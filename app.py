# -*- coding: utf-8 -*-
"""
æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V2.1
- Europe PMCå…¨æ–‡æ·±åº¦æŒ–æ˜ï¼ˆJSON APIï¼‰
- RefSeqè½¬å½•æœ¬å…¨æ™¯åˆ†æ + UniProtæ¯”å¯¹é«˜äº®
- ä¸¥æ ¼APIé€Ÿç‡é™åˆ¶ï¼ˆNCBI/UniProt/Europe PMCåˆè§„ï¼‰
- å¢å¼ºå®¹é”™æœºåˆ¶ï¼ˆè‡ªåŠ¨é€€é¿ã€åˆ†æ‰¹æŸ¥è¯¢ã€è§£æå™¨å›é€€ï¼‰
"""

import streamlit as st
import pandas as pd
import requests
from Bio import Entrez
from datetime import datetime
import time
import re
from typing import Dict, List, Optional, Tuple
import json
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import urllib.parse
from collections import Counter
import hashlib
import openai
import os
import io
import zipfile

# ==================== åˆè§„æ€§é…ç½®ä¸é€Ÿç‡é™åˆ¶ ====================
class RateLimiter:
    """APIé€Ÿç‡é™åˆ¶å™¨ - ä¸¥æ ¼éµå®ˆå„å¹³å°æ”¿ç­–"""
    def __init__(self):
        self.last_call_time = {}
        self.min_intervals = {
            'ncbi': 0.4,        # NCBI E-utilities: æ¯ç§’æœ€å¤š3æ¬¡
            'europe_pmc': 0.15, # Europe PMC: æ¯ç§’æœ€å¤š10æ¬¡
            'addgene': 1.0,     # Addgene: æ¯ç§’1æ¬¡ï¼ˆç¤¼è²Œçˆ¬è™«ï¼‰
            'uniprot': 0.5,     # UniProt: æ¯ç§’2æ¬¡
            'generic': 0.3      # é€šç”¨å»¶è¿Ÿ
        }
    
    def wait(self, service: str):
        """è¯·æ±‚å‰è°ƒç”¨ï¼Œè‡ªåŠ¨ç­‰å¾…"""
        service = service.lower()
        min_interval = self.min_intervals.get(service, self.min_intervals['generic'])
        
        now = time.time()
        last_call = self.last_call_time.get(service, 0)
        elapsed = now - last_call
        
        if elapsed < min_interval:
            sleep_time = min_interval - elapsed
            time.sleep(sleep_time)
        
        self.last_call_time[service] = time.time()

# å…¨å±€é€Ÿç‡é™åˆ¶å™¨
rate_limiter = RateLimiter()

# ==================== Streamlité…ç½® ====================
st.set_page_config(
    page_title="æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V2.1",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å®‰å…¨å¯†é’¥é…ç½®
try:
    NCBI_EMAIL = st.secrets["NCBI_EMAIL"]
    NCBI_API_KEY = st.secrets.get("NCBI_API_KEY", "")
    APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
    BAILIAN_API_KEY = st.secrets.get("BAILIAN_API_KEY") or st.secrets.get("DASHSCOPE_API_KEY", "")
    AI_MODEL = st.secrets.get("AI_MODEL", "qwen-plus")
except Exception:
    st.error("âš ï¸ è¯·å…ˆé…ç½® Secretsï¼ˆNCBI_EMAIL ç­‰ï¼‰")
    st.stop()

# åˆè§„æ€§é…ç½®
COMPLIANCE_CONFIG = {
    'app_name': 'LentiviralAssessmentTool/2.1',
    'contact_email': NCBI_EMAIL,
    'max_retries': 3,
    'backoff_factor': 2
}

# å¯†ç ä¿æŠ¤
if APP_PASSWORD:
    if 'authenticated' not in st.session_state:
        st.session_state.authenticated = False
    if not st.session_state.authenticated:
        pwd = st.text_input("ğŸ”’ è¯·è¾“å…¥è®¿é—®å¯†ç ", type="password")
        if pwd == APP_PASSWORD:
            st.session_state.authenticated = True
            st.rerun()
        elif pwd:
            st.error("å¯†ç é”™è¯¯")
        st.stop()

# åˆå§‹åŒ–session state
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'search_history' not in st.session_state:
    st.session_state.search_history = []

# ==================== Addgeneçˆ¬å–æ¨¡å—ï¼ˆå®¹é”™ç‰ˆï¼‰ ====================
class AddgeneScraper:
    """Addgeneè´¨ç²’çˆ¬å–å™¨ - å¸¦è§£æå™¨å›é€€å’Œé€Ÿç‡é™åˆ¶"""
    
    def __init__(self):
        self.base_url = "https://www.addgene.org"
        self.ua = UserAgent()
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'From': COMPLIANCE_CONFIG['contact_email']
        })
        self.rate_limiter = rate_limiter
    
    @st.cache_data(ttl=86400, show_spinner=False)
    def search_plasmids(_self, gene_symbol: str, max_results: int = 5) -> List[Dict]:
        """æœç´¢Addgeneè´¨ç²’"""
        _self.rate_limiter.wait('addgene')
        
        try:
            query = urllib.parse.quote(f"{gene_symbol}")
            search_url = f"{_self.base_url}/search/?q={query}&type=plasmid"
            headers = {'User-Agent': _self.ua.random}
            
            response = _self.session.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # è§£æå™¨å›é€€ï¼šå…ˆå°è¯•lxmlï¼Œå¤±è´¥åˆ™ç”¨html.parser
            try:
                soup = BeautifulSoup(response.text, 'lxml')
            except Exception:
                soup = BeautifulSoup(response.text, 'html.parser')
            
            plasmids = []
            
            # å¤šç§é€‰æ‹©å™¨å°è¯•
            result_items = (soup.find_all('article', class_='addgene-search-result') or 
                           soup.find_all('div', class_='search-result-item') or
                           soup.select('.plasmid-item') or
                           soup.select('[data-testid="plasmid-card"]'))
            
            for item in result_items[:max_results]:
                try:
                    plasmid_data = _self._parse_plasmid_card(item, gene_symbol)
                    if plasmid_data:
                        plasmids.append(plasmid_data)
                except Exception:
                    continue
            
            if not plasmids:
                plasmids = _self._search_by_gene_page(gene_symbol)
            
            return plasmids
            
        except Exception as e:
            st.warning(f"Addgeneæ£€ç´¢æš‚æ—¶ä¸å¯ç”¨: {str(e)[:100]}")
            return []
    
    def _parse_plasmid_card(self, card, gene_symbol: str) -> Optional[Dict]:
        """è§£æå•ä¸ªè´¨ç²’å¡ç‰‡"""
        try:
            link_tag = card.find('a', href=re.compile(r'/\d{5,6}/')) or card.find('a', href=True)
            if not link_tag:
                return None
            
            href = link_tag.get('href', '')
            plasmid_id_match = re.search(r'/(\d{5,6})/', href)
            if not plasmid_id_match:
                return None
            
            plasmid_id = plasmid_id_match.group(1)
            full_url = f"{self.base_url}{href}" if href.startswith('/') else href
            
            name_tag = card.find('h3') or card.find('h2') or card.find('a', class_='title')
            name = name_tag.get_text(strip=True) if name_tag else "Unknown"
            
            if gene_symbol.lower() not in name.lower() and gene_symbol.lower() not in card.get_text().lower():
                return None
            
            return {
                'plasmid_id': plasmid_id,
                'name': name,
                'url': full_url,
                'insert_gene': gene_symbol,
            }
            
        except Exception:
            return None
    
    def _search_by_gene_page(self, gene_symbol: str) -> List[Dict]:
        """é€šè¿‡åŸºå› ä¸“å±é¡µé¢æœç´¢"""
        self.rate_limiter.wait('addgene')
        
        try:
            gene_url = f"{self.base_url}/browse/gene/{gene_symbol}/"
            headers = {'User-Agent': self.ua.random}
            response = self.session.get(gene_url, headers=headers, timeout=8)
            
            if response.status_code != 200:
                return []
            
            try:
                soup = BeautifulSoup(response.text, 'lxml')
            except Exception:
                soup = BeautifulSoup(response.text, 'html.parser')
            
            plasmids = []
            seen_ids = set()
            
            links = soup.find_all('a', href=re.compile(r'/\d{5,6}/'))
            for link in links[:5]:
                try:
                    href = link.get('href', '')
                    match = re.search(r'/(\d{5,6})/', href)
                    if match:
                        pid = match.group(1)
                        if pid not in seen_ids:
                            seen_ids.add(pid)
                            name = link.get_text(strip=True) or f"{gene_symbol} related"
                            
                            if gene_symbol.lower() not in name.lower():
                                continue
                                
                            plasmids.append({
                                'plasmid_id': pid,
                                'name': name,
                                'url': f"{self.base_url}/{pid}/",
                                'insert_gene': gene_symbol,
                                'source': 'Gene page'
                            })
                except Exception:
                    continue
            return plasmids
        except Exception:
            return []

# ==================== HPAåŸºå› è¡¨è¾¾æ•°æ®æ¨¡å— ====================
class HPAGeneData:
    """åŸºäºæœ¬åœ°proteinatlas.tsvçš„äººç±»è›‹ç™½è¡¨è¾¾æ•°æ®æŸ¥è¯¢"""
    
    def __init__(self, tsv_path: str = "data/proteinatlas.tsv"):
        self.tsv_path = tsv_path
        self.df = None
        self.available_columns = []
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½TSVæ–‡ä»¶ï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ï¼‰"""
        try:
            if not os.path.exists(self.tsv_path):
                self._auto_download()
                if not os.path.exists(self.tsv_path):
                    return
            
            self.df = pd.read_csv(self.tsv_path, sep='\t', low_memory=False)
            self.available_columns = self.df.columns.tolist()
            self.df.columns = [col.strip() for col in self.df.columns]
            
            if 'Gene' not in self.df.columns:
                gene_col = None
                for col in self.df.columns:
                    if 'gene' in col.lower():
                        gene_col = col
                        break
                if gene_col:
                    self.df = self.df.rename(columns={gene_col: 'Gene'})
                else:
                    st.error("HPAæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åŸºå› ååˆ—")
                    self.df = pd.DataFrame()
                    return
            
            st.success(f"âœ… HPAæ•°æ®å·²åŠ è½½: {len(self.df):,}æ¡åŸºå› ")
            
        except Exception as e:
            st.error(f"åŠ è½½HPAæ•°æ®å¤±è´¥: {e}")
            self.df = pd.DataFrame()
    
    def _auto_download(self):
        """è‡ªåŠ¨ä¸‹è½½HPAæ•°æ®æ–‡ä»¶"""
        try:
            st.info("â¬‡ï¸ æ­£åœ¨ä¸‹è½½HPAæ•°æ®...")
            os.makedirs("data", exist_ok=True)
            
            url = "https://www.proteinatlas.org/download/proteinatlas.tsv.zip"
            response = requests.get(url, timeout=300)
            
            with open("data/hpa_temp.zip", "wb") as f:
                f.write(response.content)
            
            with zipfile.ZipFile("data/hpa_temp.zip", 'r') as zip_ref:
                zip_ref.extractall("data")
            
            os.remove("data/hpa_temp.zip")
            st.success("âœ… HPAæ•°æ®ä¸‹è½½å®Œæˆ")
            
        except Exception as e:
            st.error(f"ä¸‹è½½å¤±è´¥: {e}")
    
    def get_gene_data(self, gene_symbol: str) -> Dict:
        """è·å–åŸºå› æ•°æ®"""
        if self.df is None or self.df.empty:
            return {}
        
        try:
            mask = self.df['Gene'].str.upper() == gene_symbol.upper()
            if not mask.any():
                return {}
            
            row = self.df[mask].iloc[0]
            result = {}
            
            if 'Ensembl' in self.df.columns:
                result['ensembl_id'] = str(row['Ensembl'])
            if 'Uniprot' in self.df.columns:
                result['uniprot_id'] = str(row['Uniprot'])
            if 'Subcellular main location' in self.df.columns:
                result['subcellular_location'] = str(row['Subcellular main location'])
            elif 'Subcellular location' in self.df.columns:
                result['subcellular_location'] = str(row['Subcellular location'])
            
            rna_col = None
            for col in self.df.columns:
                if 'tissue' in col.lower() and 'rna' in col.lower():
                    rna_col = col
                    break
            if rna_col:
                result['rna_tissue_specificity'] = str(row[rna_col])
            
            if 'Reliability' in self.df.columns:
                result['reliability'] = str(row['Reliability'])
            else:
                result['reliability'] = 'N/A'
            
            if 'ensembl_id' in result:
                result['hpa_link'] = f"https://www.proteinatlas.org/{result['ensembl_id']}"
            
            return result
            
        except Exception as e:
            st.error(f"æŸ¥è¯¢HPAæ•°æ®é”™è¯¯: {e}")
            return {}
    
    def check_data_available(self) -> bool:
        return self.df is not None and not self.df.empty

# ==================== æ…¢ç—…æ¯’é£é™©è¯„ä¼°ç±» ====================
class LentiviralRiskAssessor:
    """æ…¢ç—…æ¯’åŒ…è£…é£é™©è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.risk_keywords = {
            "high": {
                "antiviral": ["interferon", "ifn", "antiviral", "innate immunity", "rig-i", "tlr", "sting", "mavs", "irf"],
                "toxic": ["lethal", "essential", "cell death", "apoptosis", "toxic", "fatal"],
                "proliferation": ["cell cycle arrest", "growth inhibition", "anti-proliferative", "tumor suppressor", "contact inhibition"],
                "structure": ["transmembrane domain", "secreted protein", "extracellular matrix", "collagen"]
            },
            "medium": {
                "signaling": ["kinase", "phosphatase", "signal transduction", "pathway"],
                "transcription": ["transcription factor", "nuclear receptor", "epigenetic"]
            }
        }
    
    def assess_by_function(self, gene_description: str, phenotype: str) -> Dict:
        """æ ¹æ®åŸºå› åŠŸèƒ½è¯„ä¼°é£é™©"""
        desc_lower = gene_description.lower()
        risks = []
        risk_level = "low"
        
        for level, categories in self.risk_keywords.items():
            for category, keywords in categories.items():
                matched = [kw for kw in keywords if kw in desc_lower]
                if matched:
                    risks.append(f"{category}: æ£€æµ‹åˆ°å…³é”®è¯ {matched[:2]}...")
                    if level == "high":
                        risk_level = "high"
                    elif level == "medium" and risk_level != "high":
                        risk_level = "medium"
        
        if "å¿…éœ€" in phenotype or "lethal" in phenotype.lower() or "essential" in phenotype.lower():
            risks.append("è‡´æ­»æ€§: å¿…éœ€åŸºå› ï¼Œæ•²é™¤å¯èƒ½å¯¼è‡´ç»†èƒæ­»äº¡")
            risk_level = "high"
        
        return {
            "risk_level": risk_level,
            "risks": risks,
            "recommendation": self._get_recommendation(risk_level, risks)
        }
    
    def _get_recommendation(self, risk_level: str, risks: List[str]) -> str:
        """æ ¹æ®é£é™©ç­‰çº§ç»™å‡ºå»ºè®®"""
        if risk_level == "high":
            return "âš ï¸ é«˜é£é™©ï¼šå»ºè®®ä½¿ç”¨è¯±å¯¼å‹ç³»ç»Ÿ(Tet-On/Off)æˆ–æš‚æ—¶æ€§æ•²ä½(shRNA)ï¼Œé¿å…ç›´æ¥KO"
        elif risk_level == "medium":
            return "âš¡ ä¸­ç­‰é£é™©ï¼šå¯è¿›è¡ŒKOä½†éœ€å¯†åˆ‡ç›‘æµ‹ç»†èƒçŠ¶æ€ï¼Œå»ºè®®å‡†å¤‡è¯±å¯¼å‹å¤‡é€‰æ–¹æ¡ˆ"
        else:
            return "âœ… ä½é£é™©ï¼šæ ‡å‡†KOæ–¹æ¡ˆé€‚ç”¨ï¼Œé¢„æœŸå¯è·å¾—ç¨³å®šç»†èƒç³»"
    
    def assess_by_literature(self, literature_data: Dict) -> Dict:
        """æ ¹æ®æ–‡çŒ®è¯„ä¼°åŒ…è£…å¯è¡Œæ€§"""
        oe_count = literature_data.get("overexpression", {}).get("count", 0)
        kd_count = literature_data.get("knockdown", {}).get("count", 0)
        ko_count = literature_data.get("knockout", {}).get("count", 0)
        
        evidence = {
            "overexpression": {"available": oe_count > 0, "count": oe_count, "method": "æ…¢ç—…æ¯’åŒ…è£…"},
            "knockdown": {"available": kd_count > 0, "count": kd_count, "method": "shRNA/siRNA"},
            "knockout": {"available": ko_count > 0, "count": ko_count, "method": "CRISPR/Cas9"}
        }
        
        if oe_count > 10:
            packaging_feasibility = "high"
        elif oe_count > 0:
            packaging_feasibility = "medium"
        else:
            packaging_feasibility = "unknown"
        
        return {
            "evidence": evidence,
            "packaging_feasibility": packaging_feasibility,
            "has_precedent": oe_count > 0 or kd_count > 0 or ko_count > 0
        }
    
    def extract_sequences_from_literature(self, articles: List[Dict]) -> Dict:
        """ä»æ–‡çŒ®ä¸­æå–shRNA/siRNA/sgRNAåºåˆ—"""
        sequences = {
            "shrna": [],
            "sirna": [],
            "sgrna": []
        }
        
        patterns = {
            "shrna": r'(?:shRNA|shRNA\s+sequence)[:\s]+([ACGTU]{19,23})',
            "sirna": r'(?:siRNA|siRNA\s+sequence)[:\s]+([ACGTU]{19,23})',
            "sgrna": r'(?:sgRNA|gRNA|guide\s+RNA)[:\s]+([ACGTU]{20,23})',
            "target_seq": r'target\s+sequence[:\s]+([ACGTU]{19,23})',
            "forward": r'[Ff]orward[:\s]+([ACGTU]{19,23})',
            "sense": r'[Ss]ense[:\s]+([ACGTU]{19,23})'
        }
        
        for article in articles:
            text = article.get("title", "") + " " + article.get("abstract_snippet", "")
            
            for seq_type, pattern in patterns.items():
                matches = re.findall(pattern, text)
                for match in matches:
                    seq = match[0] if isinstance(match, tuple) else match
                    seq = seq.upper().replace("U", "T")
                    
                    if len(seq) >= 19 and len(seq) <= 23 and all(c in "ATCG" for c in seq):
                        entry = {
                            "sequence": seq,
                            "pmid": article.get("pmid"),
                            "title": article.get("title", "")[:50] + "..." if len(article.get("title", "")) > 50 else article.get("title", ""),
                            "type": seq_type
                        }
                        
                        if "shrna" in seq_type.lower():
                            if not any(e["sequence"] == seq for e in sequences["shrna"]):
                                sequences["shrna"].append(entry)
                        elif "sirna" in seq_type.lower():
                            if not any(e["sequence"] == seq for e in sequences["sirna"]):
                                sequences["sirna"].append(entry)
                        elif "grna" in seq_type.lower() or "sgrna" in seq_type.lower():
                            if not any(e["sequence"] == seq for e in sequences["sgrna"]):
                                sequences["sgrna"].append(entry)
        
        for seq_type in sequences:
            sequences[seq_type] = sequences[seq_type][:5]
        
        return sequences

# ==================== Europe PMCå…¨æ–‡æ£€ç´¢æ¨¡å— ====================
class EuropePMCFetcher:
    """Europe PMC APIå®¢æˆ·ç«¯ - ä¸¥æ ¼éµå®ˆ10æ¬¡/ç§’é™åˆ¶"""
    
    def __init__(self, contact_email: str):
        self.base_url = "https://www.ebi.ac.uk/europepmc/webservices/rest"
        self.rate_limiter = rate_limiter
        self.headers = {
            'User-Agent': f'{COMPLIANCE_CONFIG["app_name"]} (mailto:{contact_email})',
            'Accept': 'application/json',
            'From': contact_email
        }
    
    def search_fulltext_articles(self, gene_symbol: str, construct_type: str, max_results: int = 5) -> List[Dict]:
        """æ£€ç´¢Europe PMCå…¨æ–‡æ–‡ç«  - ä¸¥æ ¼é™é€Ÿ"""
        self.rate_limiter.wait('europe_pmc')
        
        try:
            type_keywords = {
                "knockdown": "(shRNA OR siRNA OR knockdown)",
                "knockout": "(CRISPR OR knockout OR sgRNA)",
                "overexpression": "(overexpression OR lentiviral)"
            }
            
            query = f'{gene_symbol} AND {type_keywords.get(construct_type, "")} AND (has_reflist:y OR has_fulltext:y)'
            
            search_url = f"{self.base_url}/search"
            params = {
                'query': query,
                'format': 'json',
                'pageSize': max_results,
                'resultType': 'core'
            }
            
            response = requests.get(
                search_url, 
                params=params, 
                headers=self.headers, 
                timeout=20
            )
            
            # æ£€æŸ¥é€Ÿç‡é™åˆ¶
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 2))
                st.warning(f"Europe PMCé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…{retry_after}ç§’...")
                time.sleep(retry_after)
                return self.search_fulltext_articles(gene_symbol, construct_type, max_results)
            
            response.raise_for_status()
            data = response.json()
            
            results = []
            articles = data.get('resultList', {}).get('result', [])
            
            for article in articles:
                pmcid = article.get('pmcid')
                if pmcid and pmcid.startswith('PMC'):
                    # è·å–å…¨æ–‡è¯¦æƒ…ï¼ˆè‡ªåŠ¨é™é€Ÿï¼‰
                    self.rate_limiter.wait('europe_pmc')
                    fulltext_data = self.fetch_fulltext_details(pmcid.replace('PMC', ''))
                    
                    if fulltext_data:
                        results.append({
                            'pmcid': pmcid,
                            'title': article.get('title', 'N/A'),
                            'authors': article.get('authorString', 'N/A'),
                            'year': article.get('pubYear', 'N/A'),
                            'doi': article.get('doi', 'N/A'),
                            **fulltext_data
                        })
            
            return results
            
        except requests.exceptions.RequestException as e:
            st.error(f"Europe PMCç½‘ç»œé”™è¯¯: {str(e)[:100]}")
            return []
        except Exception as e:
            st.error(f"Europe PMCæ£€ç´¢é”™è¯¯: {str(e)[:100]}")
            return []
    
    def fetch_fulltext_details(self, pmcid: str) -> Optional[Dict]:
        """è·å–å…¨æ–‡Methodséƒ¨åˆ† - å¸¦é™é€Ÿ"""
        try:
            url = f"{self.base_url}/PMC{pmcid}/fullText"
            
            response = requests.get(
                url, 
                headers=self.headers, 
                timeout=15
            )
            
            if response.status_code == 429:
                time.sleep(2)
                response = requests.get(url, headers=self.headers, timeout=15)
            
            if response.status_code != 200:
                return None
            
            data = response.json()
            
            # æå–Methodséƒ¨åˆ†
            sections = data.get('fullText', {}).get('sections', [])
            methods_text = ""
            
            for section in sections:
                title = section.get('title', '').lower()
                if any(keyword in title for keyword in ['methods', 'materials', 'experimental']):
                    paragraphs = section.get('paragraphs', [])
                    methods_text = ' '.join([p.get('text', '') for p in paragraphs])
                    break
            
            if not methods_text:
                return None
            
            return self.parse_methods_details(methods_text)
            
        except Exception:
            return None
    
    def parse_methods_details(self, text: str) -> Dict:
        """è§£ææ–¹æ³•å­¦æ–‡æœ¬æå–å…³é”®ä¿¡æ¯"""
        text_lower = text.lower()
        
        # æå–ç»†èƒç³»
        cell_lines = []
        common_cells = [
            "hek293", "hela", "a549", "mcf7", "hct116", "u2os", "nih3t3", 
            "cos7", "hepg2", "mcf-10a", "mda-mb-231", "pc3", "du145"
        ]
        for cell in common_cells:
            if cell in text_lower:
                cell_lines.append(cell.upper())
        
        # æå–è´¨ç²’è½½ä½“
        vectors = []
        vector_patterns = [
            r'([pP][Ll][Vv][A-Za-z0-9\-\.]+)',
            r'([pP][Ll][Kk][Oo][\.\d]+)',
            r'(lenti[a-zA-Z0-9\-]+)',
            r'([pP][Cc][Dd][Hh][A-Za-z0-9\-]+)',
            r'([pP][Ll][Ee][Nn][Tt][Ii][\-]?[a-zA-Z0-9]+)',
            r'([pP][Ss][Pp][Aa][Xx]2?)',
            r'([pP][Mm][Dd]2\.?[Gg]?)',
            r'([pP][Ll][Pp][Aa]1?)',
        ]
        
        for pattern in vector_patterns:
            matches = re.findall(pattern, text)
            vectors.extend(matches)
        
        vectors = list(set([v for v in vectors if len(v) > 2]))
        
        # æå–ç­›é€‰æ ‡è®°æµ“åº¦
        selection = []
        sel_patterns = [
            r'(puromycin|g418|neomycin|blasticidin|hygromycin)[^\d]*(\d+\s*(?:Î¼g|ug|mg)/(?:ml|mL))',
            r'(\d+\s*(?:Î¼g|ug)/ml)[^\w]*(puromycin|g418)',
        ]
        for pattern in sel_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if isinstance(match, tuple):
                    sel_text = ' '.join([m for m in match if m])
                    selection.append(sel_text)
                else:
                    selection.append(match)
        selection = list(set(selection))
        
        # æå–åºåˆ—
        sequences = self.extract_sequences_advanced(text)
        
        return {
            'methods_text': text[:800] + "..." if len(text) > 800 else text,
            'cell_lines': cell_lines[:5],
            'vectors': vectors[:8],
            'selection': selection[:5],
            'sequences': sequences
        }
    
    def extract_sequences_advanced(self, text: str) -> Dict[str, List[Dict]]:
        """é«˜çº§åºåˆ—æå–"""
        results = {
            'shrna': [],
            'sirna': [],
            'sgrna': []
        }
        
        # shRNAé¶åºåˆ—+loop
        shrna_patterns = [
            (r'[Ss]h[Rr][Nn][Aa][^\n]{0,30}?([ACGTU]{19,21})[\s\-]+([ACGTU]{6,10})', 'target+loop'),
            (r'target\s+sequence[:\s]+([ACGTU]{19,21})', 'target'),
            (r'([ACGTU]{21})[\s\-]+[Tt][Cc][Aa][Aa][Gg][Aa][Gg]', 'classic_loop'),
        ]
        
        for pattern, seq_type in shrna_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    seq = match[0].upper().replace('U', 'T')
                else:
                    seq = match.upper().replace('U', 'T')
                
                if len(seq) >= 19 and all(c in 'ATCG' for c in seq):
                    results['shrna'].append({
                        'sequence': seq,
                        'type': seq_type,
                        'gc': round((seq.count('G') + seq.count('C'))/len(seq)*100, 1)
                    })
        
        # sgRNA
        sgrna_patterns = [
            r'[Ss][Gg][Rr][Nn][Aa][^\n]{0,20}?([ACGT]{20})[Gg]{2}',
            r'guide\s+RNA[:\s]+([ACGT]{20,21})',
            r'target\s+site[:\s]+([ACGT]{20})',
        ]
        for pattern in sgrna_patterns:
            matches = re.findall(pattern, text)
            for seq in matches:
                seq = seq.upper()
                if len(seq) >= 20:
                    results['sgrna'].append({
                        'sequence': seq[:20],
                        'gc': round((seq.count('G') + seq.count('C'))/len(seq)*100, 1)
                    })
        
        # å»é‡
        for key in results:
            seen = set()
            unique = []
            for item in results[key]:
                if item['sequence'] not in seen:
                    seen.add(item['sequence'])
                    unique.append(item)
            results[key] = unique[:5]
        
        return results

# ==================== NCBIæ•°æ®è·å–æ¨¡å—ï¼ˆå¢å¼ºåˆè§„ç‰ˆï¼‰ ====================
class BioDataFetcher:
    """NCBI/UniProt/è½¬å½•æœ¬æ•°æ®è·å– - ä¸¥æ ¼é™é€Ÿä¸é”™è¯¯å¤„ç†"""
    
    def __init__(self, email: str, api_key: str = ""):
        # NCBIé…ç½®ï¼ˆå¿…é¡»ï¼‰
        Entrez.email = email
        if api_key:
            Entrez.api_key = api_key
        
        self.email = email
        self.api_key = api_key
        self.uniprot_base = "https://rest.uniprot.org/uniprotkb/search.json"
        
        # ç»Ÿä¸€çš„åˆè§„è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': f'{COMPLIANCE_CONFIG["app_name"]} (mailto:{email})',
            'From': email,
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip, deflate'
        }
        
        self.addgene_scraper = AddgeneScraper()
        self.hpa_data = HPAGeneData()
        self.europe_pmc = EuropePMCFetcher(email)
        self.rate_limiter = rate_limiter
    
    def _safe_ncbi_call(self, func, *args, **kwargs):
        """å¸¦é€Ÿç‡é™åˆ¶å’Œé”™è¯¯å¤„ç†çš„NCBIè°ƒç”¨"""
        self.rate_limiter.wait('ncbi')
        
        for attempt in range(COMPLIANCE_CONFIG['max_retries']):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                error_str = str(e).lower()
                
                # æ£€æµ‹é€Ÿç‡é™åˆ¶é”™è¯¯
                if any(x in error_str for x in ['rate limit', 'too many requests', '429']):
                    wait_time = (attempt + 1) * COMPLIANCE_CONFIG['backoff_factor']
                    st.warning(f"NCBIé€Ÿç‡é™åˆ¶è§¦å‘ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                
                # æ£€æµ‹æœåŠ¡å™¨è¿‡è½½
                if any(x in error_str for x in ['server error', '503', '502', 'timeout', 'eof']):
                    wait_time = (attempt + 1) * COMPLIANCE_CONFIG['backoff_factor'] * 2
                    st.warning(f"NCBIæœåŠ¡å™¨ç¹å¿™ï¼Œç­‰å¾…{wait_time}ç§’...")
                    time.sleep(wait_time)
                    continue
                
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                raise
        
        raise Exception("NCBIè¯·æ±‚å¤šæ¬¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
    
    def get_ncbi_gene_info(self, gene_symbol: str, species: str) -> Dict:
        """è·å–NCBIåŸºå› ä¿¡æ¯ - åˆè§„ç‰ˆ"""
        try:
            term = f"{gene_symbol}[Gene Name] AND {species}[Organism]"
            
            handle = self._safe_ncbi_call(Entrez.esearch, db="gene", term=term, retmax=1)
            record = Entrez.read(handle)
            handle.close()
            
            if not record["IdList"]:
                return {"status": "not_found", "error": f"æœªæ‰¾åˆ° {gene_symbol} ({species})"}
            
            gene_id = record["IdList"][0]
            
            handle = self._safe_ncbi_call(Entrez.efetch, db="gene", id=gene_id, rettype="xml")
            gene_data = Entrez.read(handle)
            handle.close()
            
            gene_entry = gene_data[0]
            summary = gene_entry.get("Entrezgene_summary", "")
            
            lethal_keywords = ["essential", "lethal", "required for cell viability", 
                             "knockout mice die", "embryonic lethal"]
            phenotype = "éå¿…éœ€"
            if any(kw in summary.lower() for kw in lethal_keywords):
                phenotype = "å¿…éœ€ï¼ˆæ½œåœ¨è‡´æ­»é£é™©ï¼‰"
            
            return {
                "gene_id": gene_id,
                "symbol": gene_symbol,
                "species": species,
                "description": summary[:800] if summary else "æ— æè¿°",
                "phenotype": phenotype,
                "chromosome": gene_entry.get("Entrezgene_location", [{}])[0].get("Gene-location", {}).get("Gene-location_chromosome", "N/A"),
                "status": "success"
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)[:200]}
    
    def get_uniprot_info(self, gene_symbol: str, species: str) -> Dict:
        """è·å–UniProtä¿¡æ¯ - åˆè§„ç‰ˆ"""
        self.rate_limiter.wait('uniprot')
        
        try:
            species_map = {
                "Homo sapiens": ("human", 9606),
                "Mus musculus": ("mouse", 10090),
                "Rattus norvegicus": ("rat", 10116)
            }
            org_name, tax_id = species_map.get(species, (species.lower(), None))
            
            queries = [
                f"gene:{gene_symbol}+organism_id:{tax_id}",
                f"gene:{gene_symbol}+organism:{org_name}",
                f"{gene_symbol}+organism:{org_name}",
                gene_symbol
            ]
            
            for query in queries:
                try:
                    params = {
                        "query": query,
                        "fields": "accession,gene_names,length,cc_subcellular_location,sequence,protein_name",
                        "format": "json",
                        "size": 5
                    }
                    
                    response = requests.get(
                        self.uniprot_base, 
                        params=params, 
                        headers=self.headers, 
                        timeout=15
                    )
                    
                    # æ£€æŸ¥é€Ÿç‡é™åˆ¶
                    if response.status_code == 429:
                        retry_after = int(response.headers.get('Retry-After', 5))
                        st.warning(f"UniProté€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…{retry_after}ç§’...")
                        time.sleep(retry_after)
                        continue
                    
                    response.raise_for_status()
                    data = response.json()
                    
                    if data.get("results"):
                        best_match = None
                        gene_names = []
                        
                        for protein in data["results"]:
                            genes = protein.get("genes", [])
                            for g in genes:
                                if g.get("geneName"):
                                    gene_names.append(g["geneName"].get("value", "").upper())
                                if g.get("synonyms"):
                                    for syn in g["synonyms"]:
                                        gene_names.append(syn.get("value", "").upper())
                            
                            if gene_symbol.upper() in gene_names:
                                best_match = protein
                                break
                        
                        if not best_match:
                            best_match = data["results"][0]
                        
                        accession = best_match.get("primaryAccession", "")
                        seq_length = best_match.get("sequence", {}).get("length", 0)
                        
                        loc_text = ""
                        comments = best_match.get("comments", [])
                        for comment in comments:
                            if comment.get("commentType") == "SUBCELLULAR LOCATION":
                                locations = comment.get("subcellularLocations", [])
                                locs = [loc.get("location", {}).get("value", "") 
                                       for loc in locations if loc.get("location")]
                                loc_text = "; ".join([l for l in locs if l])
                        
                        cds_length = seq_length * 3 if seq_length else 0
                        
                        return {
                            "uniprot_id": accession,
                            "protein_length": seq_length,
                            "cds_length_bp": cds_length,
                            "subcellular_location": loc_text or "æœªæ ‡æ³¨",
                            "protein_name": best_match.get("proteinDescription", {}).get("recommendedName", {}).get("fullName", {}).get("value", ""),
                            "status": "success",
                            "match_type": "exact" if gene_symbol.upper() in gene_names else "partial"
                        }
                        
                except Exception:
                    continue
            
            return {
                "status": "not_found",
                "error": f"UniProt æœªæ‰¾åˆ° {gene_symbol}ï¼Œè¯·ç¡®è®¤åŸºå› ç¬¦å·"
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)[:200]}
    
    def get_uniprot_sequence(self, uniprot_id: str) -> str:
        """è·å–UniProtå®Œæ•´æ°¨åŸºé…¸åºåˆ— - åˆè§„ç‰ˆ"""
        self.rate_limiter.wait('uniprot')
        
        try:
            url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta"
            response = requests.get(url, headers=self.headers, timeout=10)
            
            if response.status_code == 429:
                time.sleep(5)
                response = requests.get(url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                lines = response.text.strip().split('\n')
                sequence = ''.join(lines[1:])
                return sequence
            return ""
        except:
            return ""
    
    def get_all_transcripts(self, gene_symbol: str, species: str) -> Tuple[List[Dict], str]:
        """è·å–æ‰€æœ‰RefSeqè½¬å½•æœ¬ - ä¸¥æ ¼é™é€Ÿä¸åˆ†æ‰¹æŸ¥è¯¢"""
        transcripts = []
        uniprot_seq = ""
        
        try:
            # 1. è·å–UniProtå‚è€ƒåºåˆ—
            try:
                uniprot_info = self.get_uniprot_info(gene_symbol, species)
                if uniprot_info.get("status") == "success":
                    uniprot_id = uniprot_info.get("uniprot_id")
                    uniprot_seq = self.get_uniprot_sequence(uniprot_id)
            except Exception as e:
                st.warning(f"UniProtåºåˆ—è·å–å¤±è´¥ï¼Œå°†è·³è¿‡æ¯”å¯¹: {str(e)[:50]}")
            
            # 2. è·å–Gene ID
            term = f"{gene_symbol}[Gene Name] AND {species}[Organism]"
            handle = self._safe_ncbi_call(Entrez.esearch, db="gene", term=term, retmax=1)
            record = Entrez.read(handle)
            handle.close()
            
            if not record["IdList"]:
                return [], uniprot_seq
            
            gene_id = record["IdList"][0]
            
            # 3. è·å–è½¬å½•æœ¬IDåˆ—è¡¨
            try:
                handle = self._safe_ncbi_call(
                    Entrez.elink, 
                    dbfrom="gene", 
                    db="nucleotide", 
                    id=gene_id, 
                    linkname="gene_refseq_rna"
                )
                link_record = Entrez.read(handle)
                handle.close()
            except Exception as e:
                st.warning(f"NCBIè¿æ¥ä¸ç¨³å®šï¼Œè·³è¿‡è½¬å½•æœ¬æ£€ç´¢: {str(e)[:80]}")
                return [], uniprot_seq
            
            transcript_ids = []
            if link_record and len(link_record) > 0:
                for link in link_record[0].get("LinkSetDb", []):
                    if link.get("LinkName") == "gene_refseq_rna":
                        for item in link.get("Link", []):
                            transcript_ids.append(item.get("Id"))
            
            if not transcript_ids:
                return [], uniprot_seq
            
            # ä¸¥æ ¼é™åˆ¶æ•°é‡ï¼ˆéµå®ˆNCBIæ”¿ç­–ï¼‰
            transcript_ids = transcript_ids[:10]
            
            # 4. åˆ†æ‰¹è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆä¸¥æ ¼éµå®ˆæ¯ç§’3æ¬¡é™åˆ¶ï¼‰
            batch_size = 3  # æ¯æ‰¹3ä¸ªï¼Œé…åˆ0.4så»¶è¿Ÿ
            for i in range(0, len(transcript_ids), batch_size):
                batch = transcript_ids[i:i+batch_size]
                
                try:
                    handle = self._safe_ncbi_call(
                        Entrez.esummary, 
                        db="nucleotide", 
                        id=",".join(batch)
                    )
                    summaries = Entrez.read(handle)
                    handle.close()
                    
                    for summary in summaries:
                        try:
                            accession = summary.get("AccessionVersion", "N/A")
                            title = summary.get("Title", "")
                            length = summary.get("Length", 0)
                            
                            if "mRNA" in title or "transcript" in title.lower():
                                tx_data = {
                                    "transcript_id": accession,
                                    "title": title[:80],
                                    "length_nt": length,
                                    "protein_length_aa": None,
                                    "match_uniprot": False,
                                    "identity_percent": 0,
                                    "is_canonical": False
                                }
                                
                                # å°è¯•ä¼°ç®—è›‹ç™½é•¿åº¦
                                if summary.get("CdStart") and summary.get("CdStop"):
                                    try:
                                        cds_len = int(summary["CdStop"]) - int(summary["CdStart"])
                                        if cds_len > 0:
                                            tx_data["protein_length_aa"] = cds_len // 3
                                    except:
                                        pass
                                
                                transcripts.append(tx_data)
                        except:
                            continue
                            
                except Exception as e:
                    st.warning(f"è½¬å½•æœ¬æ‰¹æ¬¡ {i//batch_size + 1} è·å–å¤±è´¥: {str(e)[:80]}")
                    continue
            
            # 5. æ¯”å¯¹åˆ°UniProtï¼ˆåŸºäºé•¿åº¦ç›¸ä¼¼æ€§ï¼‰
            if uniprot_seq and transcripts:
                uniprot_len = len(uniprot_seq)
                for tx in transcripts:
                    if tx["protein_length_aa"]:
                        tx_len = tx["protein_length_aa"]
                        similarity = 1 - abs(tx_len - uniprot_len) / max(tx_len, uniprot_len, 1)
                        tx["identity_percent"] = round(similarity * 100, 1)
                        
                        if similarity > 0.95:
                            tx["match_uniprot"] = True
                            tx["is_canonical"] = True
            
            # æ’åºï¼šåŒ¹é…UniProtçš„æ’åœ¨å‰é¢
            transcripts.sort(key=lambda x: (x["match_uniprot"], x["identity_percent"]), reverse=True)
            
            return transcripts, uniprot_seq
            
        except Exception as e:
            st.error(f"è½¬å½•æœ¬æ£€ç´¢é”™è¯¯: {str(e)[:100]}")
            return [], uniprot_seq
    
    def search_pmc_fulltext_europe(self, gene_symbol: str, construct_type: str, max_results: int = 5) -> List[Dict]:
        """Europe PMCå…¨æ–‡æ£€ç´¢å…¥å£"""
        return self.europe_pmc.search_fulltext_articles(gene_symbol, construct_type, max_results)

# ==================== é€šä¹‰åƒé—®AIåˆ†ææ¨¡å— ====================
class AIAnalyzer:
    def __init__(self):
        self.client = None
        self.model = None
        self.cache = {}
        
        try:
            api_key = BAILIAN_API_KEY
            
            if not api_key:
                raise ValueError("æœªé…ç½® BAILIAN_API_KEY")
            
            self.model = AI_MODEL
            
            self.client = openai.OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            
            st.success(f"âœ… é€šä¹‰åƒé—®å·²è¿æ¥ï¼ˆæ¨¡å‹ï¼š{self.model}ï¼‰")
            
        except Exception as e:
            st.error(f"é€šä¹‰åƒé—®åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            raise e
    
    def _get_cache_key(self, text: str, task: str) -> str:
        return hashlib.md5(f"{task}:{text}".encode()).hexdigest()[:16]
    
    def _call_qwen(self, prompt: str, json_mode: bool = True, temperature: float = 0.3) -> Dict:
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯èµ„æ·±ç»†èƒç”Ÿç‰©å­¦å’Œåˆ†å­å…‹éš†ä¸“å®¶ï¼Œæ“…é•¿åˆ†æåŸºå› åŠŸèƒ½ã€è®¾è®¡ç»†èƒç³»æ„å»ºå®éªŒæ–¹æ¡ˆã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸“ä¸šä¸”å…·ä½“ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": 2000
            }
            
            if json_mode:
                kwargs["response_format"] = {"type": "json_object"}
            
            with st.spinner("ğŸ¤– é€šä¹‰åƒé—®åˆ†æä¸­..."):
                response = self.client.chat.completions.create(**kwargs)
                content = response.choices[0].message.content
                
                if json_mode:
                    return json.loads(content)
                return {"result": content}
                
        except Exception as e:
            st.error(f"é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}")
            return {"error": str(e)}
    
    def assess_gene_essentiality(self, gene_data: Dict, uniprot_data: Dict) -> Dict:
        gene_name = gene_data.get("symbol", "")
        description = gene_data.get("description", "")
        location = uniprot_data.get("subcellular_location", "")
        
        cache_key = self._get_cache_key(f"{gene_name}_{description[:100]}", "essentiality")
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹åŸºå› ä¿¡æ¯ï¼Œåˆ†æè¯¥åŸºå› æ˜¯å¦ä¸ºç»†èƒå¿…éœ€åŸºå› ï¼ˆessential geneï¼‰ï¼Œå¹¶è¯„ä¼°æ„å»ºKOç»†èƒç³»çš„å¯è¡Œæ€§ã€‚

åŸºå› åç§°ï¼š{gene_name}
åŸºå› åŠŸèƒ½æè¿°ï¼š{description}
äºšç»†èƒå®šä½ï¼š{location}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_essential": "æ˜¯/å¦/å¯èƒ½",
    "confidence": "é«˜/ä¸­/ä½",
    "rationale": "è¯¦ç»†è§£é‡Šåˆ¤æ–­ç†ç”±ï¼ŒåŸºäºè¯¥åŸºå› çš„ç”Ÿç‰©å­¦åŠŸèƒ½",
    "expected_phenotype": "å¦‚æœæ•²é™¤ï¼Œé¢„æœŸä¼šå‡ºç°ä»€ä¹ˆç»†èƒè¡¨å‹ï¼ˆå¦‚ç»†èƒæ­»äº¡ã€å¢æ®–å‡æ…¢ã€å‘¨æœŸé˜»æ»ç­‰ï¼‰",
    "construction_difficulty": "å®¹æ˜“/ä¸­ç­‰/å›°éš¾",
    "recommendation": "å…·ä½“çš„å®éªŒå»ºè®®ï¼ˆå¦‚æ˜¯å¦å»ºè®®ä½¿ç”¨è¯±å¯¼å‹ç³»ç»Ÿã€å…ˆå°è¯•KDè¿˜æ˜¯ç›´æ¥KOï¼‰",
    "key_considerations": "å®éªŒè®¾è®¡ä¸­çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼ˆ2-3ç‚¹ï¼‰",
    "alternative_approaches": "å¦‚æœKOå›°éš¾ï¼Œå»ºè®®çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚ä½¿ç”¨siRNAã€è¯±å¯¼å‹æ•²é™¤ç­‰ï¼‰"
}}

æ³¨æ„ï¼š
1. å¦‚æœåŸºå› æ˜¯ç®¡å®¶åŸºå› ï¼ˆå¦‚GAPDHã€ACTBï¼‰æˆ–å‚ä¸DNAå¤åˆ¶/ä¿®å¤æ ¸å¿ƒåŠŸèƒ½ï¼Œé€šå¸¸åˆ¤å®šä¸ºå¿…éœ€
2. å¦‚æœæ˜¯ç™ŒåŸºå› æˆ–ä¿¡å·é€šè·¯åˆ†å­ï¼Œé€šå¸¸ä¸ºéå¿…éœ€ä½†å¯èƒ½æœ‰è¡¨å‹
3. è¯·ç»™å‡ºå…·ä½“ç”Ÿç‰©å­¦æœºåˆ¶è§£é‡Šï¼Œä¸è¦æ³›æ³›è€Œè°ˆ"""
        
        result = self._call_qwen(prompt, json_mode=True)
        self.cache[cache_key] = result
        return result
    
    def analyze_literature_deep(self, articles: List[Dict], gene: str, construct_type: str) -> Dict:
        if not articles:
            return {"summary": "æ— ç›¸å…³æ–‡çŒ®", "protocols": []}
        
        articles_text = "\n\n".join([
            f"æ–‡çŒ®{i+1}:\næ ‡é¢˜ï¼š{a['title']}\næ–¹æ³•å…³é”®è¯ï¼š{a['methods']}\næ‘˜è¦ç‰‡æ®µï¼š{a.get('abstract_snippet', '')}"
            for i, a in enumerate(articles[:5])
        ])
        
        cache_key = self._get_cache_key(articles_text[:500], f"lit_{construct_type}")
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        prompt = f"""ä½œä¸ºæ–¹æ³•å­¦ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹å…³äº"{gene}"åŸºå› {construct_type}ï¼ˆè¿‡è¡¨è¾¾/æ•²ä½/æ•²é™¤ï¼‰ç»†èƒç³»æ„å»ºçš„æ–‡çŒ®ã€‚

{articles_text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "summary": "è¿™äº›ç ”ç©¶çš„æ€»ä½“æ–¹æ³•å­¦è¶‹åŠ¿ï¼ˆä¸­æ–‡ï¼Œ2-3å¥è¯æ¦‚æ‹¬ï¼‰",
    "protocols": [
        {{
            "cell_line": "ä½¿ç”¨çš„ç»†èƒç³»",
            "vector_system": "å…·ä½“è½½ä½“åç§°ï¼ˆå¦‚pLKO.1, lentiCRISPRv2, pLVXç­‰ï¼‰",
            "delivery_method": "é€’é€æ–¹æ³•ï¼ˆå¦‚æ…¢ç—…æ¯’è½¬å¯¼ã€è„‚è´¨ä½“è½¬æŸ“Lipofectamine 2000/3000ã€ç”µè½¬ç­‰ï¼‰",
            "selection_method": "ç­›é€‰æ–¹æ³•å’Œæµ“åº¦ï¼ˆå¦‚Puromycin 2Î¼g/mL, G418 500Î¼g/mLç­‰ï¼‰",
            "validation_method": "éªŒè¯æ–¹æ³•ï¼ˆå¦‚Western Blotã€qPCRã€å…ç–«è§å…‰ã€æµå¼ç­‰ï¼‰",
            "efficiency": "æ•ˆç‡ä¿¡æ¯ï¼ˆå¦‚æœ‰æåŠï¼‰",
            "duration": "å®éªŒå‘¨æœŸï¼ˆå¦‚æœ‰æåŠï¼‰"
        }}
    ],
    "common_methods": "æœ€å¸¸è§çš„æ–¹æ³•ç»„åˆï¼ˆä¸­æ–‡æ€»ç»“ï¼‰",
    "critical_factors": "å½±å“å®éªŒæˆåŠŸçš„å…³é”®å› ç´ ï¼ˆ2-3ç‚¹ï¼‰",
    "troubleshooting": "å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ±¡æŸ“ã€æ•ˆç‡ä½ã€ç»†èƒæ­»äº¡ç­‰ï¼‰",
    "recommendation": "é’ˆå¯¹è¯¥åŸºå› {construct_type}çš„å…·ä½“æ“ä½œå»ºè®®"
}}

æ³¨æ„ï¼š
1. å¦‚æœæŸå­—æ®µåœ¨æ–‡çŒ®ä¸­æœªæåŠï¼Œå¡«å†™"æœªæåŠ"
2. ä¼˜å…ˆæå–å…·ä½“çš„è¯•å‰‚åç§°å’Œæµ“åº¦ï¼ˆå¦‚Lipofectamine 2000, Polybrene 8Î¼g/mLç­‰ï¼‰
3. å¦‚æœæåˆ°å¤šç§ç»†èƒç³»ï¼Œåˆ—å‡ºæœ€å¸¸ç”¨çš„2-3ç§"""
        
        result = self._call_qwen(prompt, json_mode=True)
        self.cache[cache_key] = result
        return result

# ==================== åˆ†æä¸»ç±» ====================
class ConstructAnalyzer:
    def __init__(self):
        self.fetcher = BioDataFetcher(NCBI_EMAIL, NCBI_API_KEY)
        self.risk_assessor = LentiviralRiskAssessor()
    
    def analyze_gene(self, gene_symbol: str, species: str, 
                    cell_line: Optional[str] = None, 
                    cell_species: Optional[str] = None) -> Dict:
        
        with st.spinner(f"ğŸ” æ­£åœ¨æ·±åº¦åˆ†æ {gene_symbol}..."):
            
            st.text("æ£€ç´¢ NCBI Gene...")
            ncbi_info = self.fetcher.get_ncbi_gene_info(gene_symbol, species)
            time.sleep(0.1)
            
            st.text("æ£€ç´¢ UniProt...")
            uniprot_info = self.fetcher.get_uniprot_info(gene_symbol, species)
            time.sleep(0.1)
            
            st.text("æ£€ç´¢ RefSeq è½¬å½•æœ¬...")
            transcripts, uniprot_seq = self.fetcher.get_all_transcripts(gene_symbol, species)
            time.sleep(0.1)
            
            st.text("æ£€ç´¢ Addgene...")
            addgene_plasmids = self.fetcher.addgene_scraper.search_plasmids(gene_symbol)
            time.sleep(0.1)
            
            hpa_gene_data = {}
            if species == "Homo sapiens":
                st.text("æ£€ç´¢ HPA è›‹ç™½è¡¨è¾¾æ•°æ®...")
                hpa_gene_data = self.fetcher.hpa_data.get_gene_data(gene_symbol)
            time.sleep(0.1)
            
            st.text("æ£€ç´¢ PubMed æ–‡çŒ®...")
            literature = self._search_all_constructs(gene_symbol, cell_line)
            time.sleep(0.1)
            
            st.text("Europe PMC å…¨æ–‡æ£€ç´¢...")
            europe_pmc_data = {}
            for ctype in ["knockdown", "knockout"]:
                europe_pmc_data[ctype] = self.fetcher.search_pmc_fulltext_europe(
                    gene_symbol, ctype, max_results=5
                )
            time.sleep(0.1)
            
            st.text("è¯„ä¼°æ…¢ç—…æ¯’é£é™©...")
            lentiviral = self._assess_lentiviral_comprehensive(
                gene_symbol, ncbi_info, uniprot_info, literature
            )
            
            ai_analysis = {}
            try:
                ai_analyzer = AIAnalyzer()
                
                with st.spinner("ğŸ¤– é€šä¹‰åƒé—®æ­£åœ¨æ·±åº¦åˆ†æ..."):
                    ai_analysis["gene_assessment"] = ai_analyzer.assess_gene_essentiality(
                        ncbi_info, uniprot_info
                    )
                    
                    if cell_line and literature.get("specific_cell", {}).get("found"):
                        for ctype in ["overexpression", "knockdown", "knockout"]:
                            if literature["specific_cell"][ctype]["articles"]:
                                ai_result = ai_analyzer.analyze_literature_deep(
                                    literature["specific_cell"][ctype]["articles"], gene_symbol, ctype
                                )
                                literature["specific_cell"][ctype]["ai_analysis"] = ai_result
                    
                    for ctype in ["overexpression", "knockdown", "knockout"]:
                        if literature[ctype]["articles"]:
                            ai_result = ai_analyzer.analyze_literature_deep(
                                literature[ctype]["articles"], gene_symbol, ctype
                            )
                            literature[ctype]["ai_analysis"] = ai_result
                    
                    ai_analysis["enabled"] = True
                    
            except Exception as e:
                st.warning(f"AIåˆ†ææœªå¯ç”¨ï¼š{e}")
                ai_analysis["enabled"] = False
            
            result = {
                "input_info": {
                    "gene_symbol": gene_symbol,
                    "species": species,
                    "cell_line": cell_line or "æœªæŒ‡å®š",
                    "cell_species": cell_species or "æœªæŒ‡å®š",
                    "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
                    "ai_enabled": ai_analysis.get("enabled", False)
                },
                "gene_function": ncbi_info,
                "protein_data": uniprot_info,
                "transcripts": transcripts,
                "uniprot_sequence_length": len(uniprot_seq),
                "hpa_gene_data": hpa_gene_data,
                "addgene_plasmids": addgene_plasmids,
                "lentiviral_assessment": lentiviral,
                "cell_line_constructs": literature,
                "europe_pmc_fulltext": europe_pmc_data,
                "ai_analysis": ai_analysis,
                "database_record": self._format_database_record(
                    gene_symbol, species, cell_line, ncbi_info, uniprot_info, 
                    lentiviral, literature, addgene_plasmids, hpa_gene_data
                )
            }
            
            return result
    
    def _assess_lentiviral_comprehensive(self, gene_symbol: str, ncbi_info: Dict, 
                                        uniprot_info: Dict, literature: Dict) -> Dict:
        """ç»¼åˆè¯„ä¼°æ…¢ç—…æ¯’é€‚ç”¨æ€§"""
        cds_len = uniprot_info.get("cds_length_bp", 0)
        
        # 1. CDSé•¿åº¦è¯„ä¼°
        if cds_len > 2500:
            cds_risk = {"level": "high", "suitable": False, "reason": f"CDSè¿‡é•¿({cds_len}bp)ï¼Œè¿œè¶…2000bpæ¨èå€¼"}
        elif cds_len > 2000:
            cds_risk = {"level": "medium", "suitable": True, "reason": f"CDSè¾ƒé•¿({cds_len}bp)ï¼Œå»ºè®®ä½¿ç”¨ç¬¬ä¸‰ä»£ç³»ç»Ÿ"}
        elif cds_len == 0:
            cds_risk = {"level": "unknown", "suitable": True, "reason": "æ— æ³•è·å–CDSé•¿åº¦ä¿¡æ¯"}
        else:
            cds_risk = {"level": "low", "suitable": True, "reason": f"CDSé•¿åº¦åˆé€‚({cds_len}bp)"}
        
        # 2. åŸºå› åŠŸèƒ½é£é™©è¯„ä¼°
        function_risk = self.risk_assessor.assess_by_function(
            ncbi_info.get("description", ""),
            ncbi_info.get("phenotype", "")
        )
        
        # 3. æ–‡çŒ®è¯æ®è¯„ä¼°
        lit_evidence = self.risk_assessor.assess_by_literature(literature)
        
        # 4. æå–åºåˆ—
        sequences = {}
        for ctype in ["knockdown", "knockout"]:
            if literature[ctype].get("articles"):
                seqs = self.risk_assessor.extract_sequences_from_literature(
                    literature[ctype]["articles"]
                )
                if any(seqs.values()):
                    sequences[ctype] = seqs
        
        # ç»¼åˆå»ºè®®
        recommendations = []
        warnings = []
        
        if cds_risk["level"] == "high":
            warnings.append(f"âš ï¸ é•¿åº¦é£é™©: {cds_risk['reason']}")
            recommendations.append("å»ºè®®ä½¿ç”¨split-vectorç³»ç»Ÿæˆ–é€‰æ‹©å…¶ä»–é€’é€æ–¹å¼ï¼ˆå¦‚è½¬åº§å­ï¼‰")
        elif cds_risk["level"] == "medium":
            warnings.append(f"âš¡ é•¿åº¦è­¦å‘Š: {cds_risk['reason']}")
        
        if function_risk["risk_level"] == "high":
            warnings.append(f"ğŸš¨ åŠŸèƒ½é£é™©: {', '.join(function_risk['risks'][:2])}")
            recommendations.append("å¼ºçƒˆå»ºè®®ä½¿ç”¨è¯±å¯¼å‹è¡¨è¾¾ç³»ç»Ÿï¼ˆTet-On/Offï¼‰")
        elif function_risk["risk_level"] == "medium":
            warnings.append(f"âš¡ åŠŸèƒ½æ³¨æ„: {', '.join(function_risk['risks'][:1])}")
        
        if not lit_evidence["has_precedent"]:
            warnings.append("ğŸ“š æ–‡çŒ®ç¼ºä¹: æœªæ‰¾åˆ°è¯¥åŸºå› çš„ç—…æ¯’åŒ…è£…æ–‡çŒ®è®°å½•")
            recommendations.append("å»ºè®®å…ˆè¿›è¡Œå°è§„æ¨¡åŒ…è£…æµ‹è¯•")
        else:
            if lit_evidence["evidence"]["overexpression"]["available"]:
                recommendations.append("âœ… æ–‡çŒ®æ”¯æŒ: å·²æœ‰æˆåŠŸè¿‡è¡¨è¾¾è®°å½•")
        
        # æ€»ä½“è¯„çº§
        if cds_risk["level"] == "high" or function_risk["risk_level"] == "high":
            overall_rating = "âŒ é«˜é£é™©ï¼ˆä¸æ¨èæ ‡å‡†æ–¹æ¡ˆï¼‰"
        elif cds_risk["level"] == "medium" or function_risk["risk_level"] == "medium":
            overall_rating = "âš ï¸ ä¸­ç­‰é£é™©ï¼ˆéœ€ä¼˜åŒ–æ–¹æ¡ˆï¼‰"
        else:
            overall_rating = "âœ… ä½é£é™©ï¼ˆæ ‡å‡†æ–¹æ¡ˆé€‚ç”¨ï¼‰"
        
        return {
            "cds_assessment": cds_risk,
            "function_risk": function_risk,
            "literature_evidence": lit_evidence,
            "sequences": sequences,
            "warnings": warnings,
            "recommendations": recommendations,
            "overall_rating": overall_rating,
            "overall_suitable": cds_risk["suitable"] and function_risk["risk_level"] != "high"
        }
    
    def _search_all_constructs(self, gene_symbol: str, cell_line: Optional[str]) -> Dict:
        """åŒåˆ†æ”¯æ–‡çŒ®æ£€ç´¢"""
        results = {}
        
        # åˆ†æ”¯1ï¼šç‰¹å®šç»†èƒç³»
        if cell_line:
            query_specific = f'{gene_symbol}[Title/Abstract] AND {cell_line}[Title/Abstract] AND (cell line OR cell-line)'
            
            try:
                handle = self.fetcher._safe_ncbi_call(
                    Entrez.esearch, 
                    db="pubmed", 
                    term=query_specific, 
                    retmax=100, 
                    sort="relevance"
                )
                record = Entrez.read(handle)
                handle.close()
                
                pmids = record["IdList"]
                
                if pmids:
                    fetch_ids = pmids[:20]
                    handle = self.fetcher._safe_ncbi_call(
                        Entrez.efetch, 
                        db="pubmed", 
                        id=fetch_ids, 
                        rettype="abstract", 
                        retmode="xml"
                    )
                    articles = Entrez.read(handle)
                    handle.close()
                    
                    specific_oe, specific_kd, specific_ko = [], [], []
                    
                    for article in articles.get("PubmedArticle", []):
                        try:
                            medline = article["MedlineCitation"]
                            article_data = medline["Article"]
                            title = article_data.get("ArticleTitle", "").lower()
                            abstract = str(article_data.get("Abstract", {}).get("AbstractText", "")).lower()
                            full_text = title + " " + abstract
                            
                            pmid = str(medline.get("PMID", "N/A"))
                            title_display = article_data.get("ArticleTitle", "N/A")
                            
                            methods = [kw for kw in ["lentiviral", "transfection", "electroporation", "transduction"] if kw in full_text]
                            
                            article_info = {
                                "pmid": pmid,
                                "title": title_display,
                                "methods": ", ".join(methods) if methods else "æœªæ˜ç¡®æåŠ",
                                "cell_line": cell_line,
                                "abstract_snippet": abstract[:300] if len(abstract) > 300 else abstract
                            }
                            
                            if any(kw in full_text for kw in ["overexpression", "over-expression", "ectopic"]):
                                specific_oe.append(article_info)
                            elif any(kw in full_text for kw in ["knockdown", "sirna", "shrna"]):
                                specific_kd.append(article_info)
                            elif any(kw in full_text for kw in ["knockout", "crispr", "knock-out"]):
                                specific_ko.append(article_info)
                                    
                        except Exception:
                            continue
                    
                    results["specific_cell"] = {
                        "found": True,
                        "total_count": len(pmids),
                        "overexpression": {"articles": specific_oe[:5], "count": len(specific_oe)},
                        "knockdown": {"articles": specific_kd[:5], "count": len(specific_kd)},
                        "knockout": {"articles": specific_ko[:5], "count": len(specific_ko)},
                        "message": f"åœ¨ {cell_line} ä¸­æ‰¾åˆ° {len(pmids)} ç¯‡æ–‡çŒ®"
                    }
                else:
                    results["specific_cell"] = {"found": False, "message": f"æœªåœ¨ {cell_line} ä¸­æ‰¾åˆ°ç›¸å…³ç ”ç©¶"}
            except Exception as e:
                results["specific_cell"] = {"found": False, "message": f"æ£€ç´¢å¤±è´¥: {e}"}
        else:
            results["specific_cell"] = {"found": False, "message": "æœªè¾“å…¥ç»†èƒç³»åç§°"}
        
        # åˆ†æ”¯2ï¼šé€šç”¨æ–‡çŒ®
        for construct_type in ["overexpression", "knockdown", "knockout"]:
            type_map = {
                "overexpression": "overexpression OR over-expression OR ectopic",
                "knockdown": "knockdown OR siRNA OR shRNA",
                "knockout": "knockout OR CRISPR OR knock-out"
            }
            
            query = f'{gene_symbol}[Title/Abstract] AND ({type_map[construct_type]}) AND (cell line OR cell-line)'
            
            try:
                handle = self.fetcher._safe_ncbi_call(
                    Entrez.esearch, 
                    db="pubmed", 
                    term=query, 
                    retmax=100, 
                    sort="relevance"
                )
                record = Entrez.read(handle)
                handle.close()
                
                pmids = record["IdList"]
                articles_list = []
                
                if pmids:
                    fetch_ids = pmids[:10]
                    handle = self.fetcher._safe_ncbi_call(
                        Entrez.efetch, 
                        db="pubmed", 
                        id=fetch_ids, 
                        rettype="abstract", 
                        retmode="xml"
                    )
                    articles = Entrez.read(handle)
                    handle.close()
                    
                    for article in articles.get("PubmedArticle", []):
                        try:
                            medline = article["MedlineCitation"]
                            article_data = medline["Article"]
                            title = article_data.get("ArticleTitle", "N/A")
                            pmid = str(medline.get("PMID", "N/A"))
                            abstract = str(article_data.get("Abstract", {}).get("AbstractText", ""))
                            
                            methods = [kw for kw in ["lentiviral", "transfection", "electroporation"] 
                                      if kw in (title + abstract).lower()]
                            
                            articles_list.append({
                                "pmid": pmid,
                                "title": title,
                                "methods": ", ".join(methods) if methods else "æœªæ˜ç¡®æåŠ",
                                "abstract_snippet": abstract[:300] + "..." if len(abstract) > 300 else abstract
                            })
                        except Exception:
                            continue
                
                results[construct_type] = {"count": len(pmids), "articles": articles_list}
                
            except Exception as e:
                results[construct_type] = {"count": 0, "articles": [], "error": str(e)}
        
        return results
    
    def _format_database_record(self, gene_symbol: str, species: str, cell_line: Optional[str],
                               ncbi_info: Dict, uniprot_info: Dict, lentiviral: Dict,
                               literature: Dict, plasmids: List, hpa_gene_data: Dict) -> Dict:
        """æ ¼å¼åŒ–æ•°æ®åº“è®°å½•"""
        return {
            "gene_symbol": gene_symbol,
            "species": species,
            "cell_line": cell_line,
            "gene_id": ncbi_info.get("gene_id"),
            "uniprot_id": uniprot_info.get("uniprot_id"),
            "ensembl_id": hpa_gene_data.get("ensembl_id"),
            "cds_length": uniprot_info.get("cds_length_bp"),
            "is_essential": ncbi_info.get("phenotype") == "å¿…éœ€ï¼ˆæ½œåœ¨è‡´æ­»é£é™©ï¼‰",
            "lentiviral_suitable": lentiviral.get("overall_suitable"),
            "lentiviral_risk": lentiviral.get("function_risk", {}).get("risk_level"),
            "literature_count": {
                "oe": literature.get("overexpression", {}).get("count", 0),
                "kd": literature.get("knockdown", {}).get("count", 0),
                "ko": literature.get("knockout", {}).get("count", 0)
            },
            "plasmid_count": len(plasmids),
            "hpa_data_available": bool(hpa_gene_data),
            "timestamp": datetime.now().isoformat()
        }

# ==================== Streamlit UI ====================
def main():
    st.title("ğŸ”¬ æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V2.1")
    st.markdown("**Europe PMCå…¨æ–‡æŒ–æ˜ + RefSeqè½¬å½•æœ¬å…¨æ™¯åˆ†æ + ä¸¥æ ¼APIåˆè§„**")
    
    with st.sidebar:
        st.header("âš™ï¸ åˆ†æå‚æ•°è®¾ç½®")
        
        gene_symbol = st.text_input("åŸºå› ç¬¦å· (Gene Symbol)", "TP53").strip().upper()
        
        species = st.selectbox(
            "åŸºå› æ¥æºç‰©ç§",
            ["Homo sapiens", "Mus musculus", "Rattus norvegicus"],
            index=0
        )
        
        cell_line = st.text_input("ç›®çš„ç»†èƒç³» (å¯é€‰)", "HEK293").strip()
        
        cell_species = st.selectbox(
            "ç»†èƒç³»ç‰©ç§",
            ["æœªæŒ‡å®š", "Human", "Mouse", "Rat", "Other"],
            index=0
        )
        
        analyze_btn = st.button("ğŸš€ å¼€å§‹æ·±åº¦åˆ†æ", type="primary", use_container_width=True)
        
        st.divider()
        
        hpa_checker = HPAGeneData()
        if hpa_checker.check_data_available():
            st.success("âœ… HPAäººç±»è›‹ç™½æ•°æ®å·²åŠ è½½")
        else:
            st.warning("âš ï¸ HPAæ•°æ®æœªé…ç½®")
            st.caption("é¦–æ¬¡ä½¿ç”¨å°†è‡ªåŠ¨ä¸‹è½½ï¼ˆçº¦30MBï¼‰")
        
        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        - âœ… Europe PMCå…¨æ–‡MethodsæŒ–æ˜
        - âœ… RefSeqè½¬å½•æœ¬å…¨æ™¯ï¼ˆUniProtæ¯”å¯¹é«˜äº®ï¼‰
        - âœ… ä¸¥æ ¼APIé€Ÿç‡é™åˆ¶ï¼ˆåˆè§„ï¼‰
        - âœ… è‡ªåŠ¨é”™è¯¯é€€é¿ä¸é‡è¯•
        """)
    
    if analyze_btn and gene_symbol:
        analyzer = ConstructAnalyzer()
        
        try:
            result = analyzer.analyze_gene(
                gene_symbol=gene_symbol,
                species=species,
                cell_line=cell_line if cell_line else None,
                cell_species=cell_species if cell_species != "æœªæŒ‡å®š" else None
            )
            
            st.session_state.analysis_results = result
            st.session_state.search_history.append({
                "gene": gene_symbol,
                "cell": cell_line,
                "time": datetime.now().strftime("%H:%M")
            })
            
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            st.exception(e)
    
    if st.session_state.analysis_results:
        display_results(st.session_state.analysis_results)

def display_results(result: Dict):
    """å±•ç¤ºåˆ†æç»“æœ"""
    info = result["input_info"]
    
    st.header(f"ğŸ“Š {info['gene_symbol']} åˆ†ææŠ¥å‘Š")
    cols = st.columns(4)
    cols[0].metric("åŸºå› ", info['gene_symbol'])
    cols[1].metric("ç‰©ç§", info['species'].split()[0])
    cols[2].metric("ç»†èƒç³»", info['cell_line'])
    cols[3].metric("AIåˆ†æ", "å·²å¯ç”¨" if info['ai_enabled'] else "æœªå¯ç”¨")
    
    tabs = st.tabs(["ğŸ§¬ åŸºå› åŠŸèƒ½ä¸è½¬å½•æœ¬", "ğŸ¦  æ…¢ç—…æ¯’é£é™©è¯„ä¼°", "ğŸ“š æ–‡çŒ®ä¸åºåˆ—", "ğŸ”¬ Europe PMCå…¨æ–‡", "ğŸ§ª å®éªŒèµ„æº"])
    
    # Tab 1: åŸºå› åŠŸèƒ½ä¸è½¬å½•æœ¬
    with tabs[0]:
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.subheader("åŸºç¡€ä¿¡æ¯")
            
            gene_data = result["gene_function"]
            if gene_data.get("status") == "success":
                st.markdown("**NCBI Gene**")
                st.write(f"åŸºå› ID: {gene_data.get('gene_id')}")
                st.write(f"æŸ“è‰²ä½“: {gene_data.get('chromosome')}")
                st.write(f"å¿…éœ€æ€§: {gene_data.get('phenotype')}")
            
            st.divider()
            
            prot_data = result["protein_data"]
            if prot_data.get("status") == "success":
                st.markdown("**UniProt**")
                st.write(f"ID: {prot_data.get('uniprot_id')}")
                st.write(f"è›‹ç™½é•¿åº¦: {prot_data.get('protein_length')} aa")
                st.write(f"CDSé•¿åº¦: {prot_data.get('cds_length_bp')} bp")
                
                if result.get('uniprot_sequence_length'):
                    st.caption(f"å‚è€ƒåºåˆ—é•¿åº¦: {result['uniprot_sequence_length']} aa")
            
            st.divider()
            
            hpa_data = result.get("hpa_gene_data", {})
            if hpa_data:
                st.markdown("**HPAè¡¨è¾¾**")
                st.write(f"å¯é æ€§: {hpa_data.get('reliability', 'N/A')}")
                st.caption(f"[æŸ¥çœ‹HPA]({hpa_data.get('hpa_link', '#')})")
        
        with col2:
            st.subheader("ğŸ“‹ RefSeqè½¬å½•æœ¬å…¨æ™¯åˆ†æ")
            
            transcripts = result.get("transcripts", [])
            if transcripts:
                df_tx = pd.DataFrame(transcripts)
                
                # é«˜äº®åŒ¹é…UniProtçš„è¡Œ
                def highlight_matches(row):
                    if row['match_uniprot']:
                        return ['background-color: rgba(75, 192, 192, 0.3); font-weight: bold'] * len(row)
                    return [''] * len(row)
                
                styled_df = df_tx.style.apply(highlight_matches, axis=1)
                
                st.dataframe(
                    styled_df,
                    column_config={
                        "transcript_id": st.column_config.TextColumn("è½¬å½•æœ¬ID", width="medium"),
                        "title": st.column_config.TextColumn("æè¿°", width="large"),
                        "length_nt": st.column_config.NumberColumn("é•¿åº¦(nt)"),
                        "protein_length_aa": st.column_config.NumberColumn("è›‹ç™½(aa)"),
                        "match_uniprot": st.column_config.CheckboxColumn("åŒ¹é…UniProt"),
                        "is_canonical": st.column_config.CheckboxColumn("ç»å…¸è½¬å½•æœ¬"),
                        "identity_percent": st.column_config.ProgressColumn("ä¸€è‡´æ€§%", format="%.1f%%", min_value=0, max_value=100)
                    },
                    use_container_width=True,
                    hide_index=True
                )
                
                match_count = sum(1 for t in transcripts if t.get('match_uniprot'))
                st.caption(f"å…±{len(transcripts)}ä¸ªè½¬å½•æœ¬ï¼Œ{match_count}ä¸ªä¸UniProtç»å…¸åºåˆ—é«˜åº¦åŒ¹é…ï¼ˆç»¿è‰²é«˜äº®ï¼‰")
                
                csv = df_tx.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½è½¬å½•æœ¬ä¿¡æ¯",
                    data=csv,
                    file_name=f"{info['gene_symbol']}_transcripts.csv",
                    mime="text/csv"
                )
            else:
                st.info("æœªæ‰¾åˆ°RefSeqè½¬å½•æœ¬ä¿¡æ¯")
    
    # Tab 2: æ…¢ç—…æ¯’é£é™©è¯„ä¼°
    with tabs[1]:
        lv = result["lentiviral_assessment"]
        
        st.subheader("ç»¼åˆé£é™©è¯„ä¼°")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("CDSé•¿åº¦é£é™©", lv['cds_assessment']['level'].upper(), 
                   help=lv['cds_assessment']['reason'])
        col2.metric("åŠŸèƒ½é£é™©ç­‰çº§", lv['function_risk']['risk_level'].upper(),
                   help="åŸºäºåŸºå› åŠŸèƒ½æè¿°çš„é£é™©è¯„ä¼°")
        col3.metric("æ€»ä½“è¯„çº§", lv['overall_rating'].split()[0])
        
        if lv['warnings']:
            st.error("**âš ï¸ é£é™©æç¤º:**")
            for warning in lv['warnings']:
                st.write(f"- {warning}")
        
        if lv['recommendations']:
            st.success("**ğŸ’¡ ä¸“å®¶å»ºè®®:**")
            for rec in lv['recommendations']:
                st.write(f"- {rec}")
        
        with st.expander("æŸ¥çœ‹åŠŸèƒ½é£é™©è¯¦æƒ…"):
            if lv['function_risk']['risks']:
                for risk in lv['function_risk']['risks']:
                    st.write(f"- {risk}")
            else:
                st.write("æœªæ£€æµ‹åˆ°ç‰¹æ®ŠåŠŸèƒ½é£é™©")
            st.info(f"**ç­–ç•¥å»ºè®®:** {lv['function_risk']['recommendation']}")
        
        with st.expander("æŸ¥çœ‹æ–‡çŒ®åŒ…è£…è¯æ®"):
            ev = lv['literature_evidence']['evidence']
            for construct, data in ev.items():
                status = "âœ…" if data['available'] else "âŒ"
                st.write(f"{status} **{construct}**: {data['count']}ç¯‡æ–‡çŒ® ({data['method']})")
    
    # Tab 3: æ–‡çŒ®ä¸åºåˆ—ï¼ˆPubMedæ‘˜è¦çº§ï¼‰
    with tabs[2]:
        literature = result["cell_line_constructs"]
        lv = result["lentiviral_assessment"]
        
        if literature.get("specific_cell", {}).get("found"):
            st.success(literature["specific_cell"]["message"])
            
            subtabs = st.tabs(["è¿‡è¡¨è¾¾", "æ•²ä½", "æ•²é™¤"])
            types_map = ["overexpression", "knockdown", "knockout"]
            
            for tab, ctype in zip(subtabs, types_map):
                with tab:
                    data = literature["specific_cell"][ctype]
                    st.write(f"æ‰¾åˆ° {data['count']} ç¯‡ç›¸å…³æ–‡çŒ®")
                    
                    if data["articles"]:
                        for article in data["articles"]:
                            with st.expander(f"{article['title'][:80]}..."):
                                st.write(f"**PMID:** {article['pmid']}")
                                st.write(f"**æ–¹æ³•:** {article['methods']}")
                    
                    if "ai_analysis" in data:
                        st.markdown("**ğŸ¤– AI æ–¹æ³•å­¦åˆ†æ**")
                        ai_data = data["ai_analysis"]
                        st.write(ai_data.get("summary", ""))
        
        if lv.get('sequences'):
            st.divider()
            st.subheader("ğŸ§¬ æ–‡çŒ®æŠ¥é“çš„é¶ç‚¹åºåˆ—")
            
            all_sequences = []
            
            for construct_type, seqs in lv['sequences'].items():
                for seq_type, entries in seqs.items():
                    for entry in entries:
                        seq = entry['sequence']
                        all_sequences.append({
                            "é¶ç‚¹ç±»å‹": seq_type.upper(),
                            "æ„å»ºç±»å‹": construct_type.upper(),
                            "åºåˆ— (5'-3')": seq,
                            "é•¿åº¦(bp)": len(seq),
                            "GCå«é‡(%)": round((seq.count('G') + seq.count('C')) / len(seq) * 100, 1),
                            "æ¥æºPMID": entry.get('pmid', 'N/A'),
                            "æ–‡çŒ®æ ‡é¢˜": entry.get('title', '')[:60],
                        })
            
            if all_sequences:
                df_seqs = pd.DataFrame(all_sequences)
                
                st.dataframe(
                    df_seqs,
                    column_config={
                        "åºåˆ— (5'-3')": st.column_config.TextColumn(width="large"),
                        "æ¥æºPMID": st.column_config.LinkColumn(help="ç‚¹å‡»è®¿é—®PubMed", display_text="æŸ¥çœ‹æ–‡çŒ®"),
                        "GCå«é‡(%)": st.column_config.NumberColumn(help="å»ºè®®40-60%")
                    },
                    use_container_width=True,
                    hide_index=True
                )
                
                col_dl1, col_dl2 = st.columns(2)
                with col_dl1:
                    csv = df_seqs.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ CSV",
                        data=csv,
                        file_name=f"{info['gene_symbol']}_é¶ç‚¹åºåˆ—.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                with col_dl2:
                    try:
                        excel_buffer = io.BytesIO()
                        df_seqs.to_excel(excel_buffer, index=False, engine='openpyxl')
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½ Excel",
                            data=excel_buffer.getvalue(),
                            file_name=f"{info['gene_symbol']}_é¶ç‚¹åºåˆ—.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                    except:
                        st.info("Excelå¯¼å‡ºéœ€å®‰è£…openpyxl")
                
                st.caption(f"å…±æå–åˆ° {len(df_seqs)} æ¡åºåˆ— | æ•°æ®æ¥æºï¼šPubMedæ–‡çŒ®æ–‡æœ¬æŒ–æ˜")
    
    # Tab 4: Europe PMCå…¨æ–‡åˆ†æ
    with tabs[3]:
        st.subheader("ğŸ”¬ Europe PMC å…¨æ–‡æ–¹æ³•å­¦æŒ–æ˜")
        st.caption("ä»å…è´¹å…¨æ–‡Methodséƒ¨åˆ†æå–è´¨ç²’ã€ç»†èƒç³»ã€åºåˆ—ç­‰è¯¦ç»†ä¿¡æ¯")
        
        europe_data = result.get("europe_pmc_fulltext", {})
        
        if not europe_data or not any(europe_data.values()):
            st.info("æœªæ£€ç´¢åˆ°Europe PMCå…¨æ–‡æ•°æ®")
        else:
            for construct_type in ["knockdown", "knockout"]:
                articles = europe_data.get(construct_type, [])
                if not articles:
                    continue
                
                with st.expander(f"{construct_type.upper()} - æ‰¾åˆ° {len(articles)} ç¯‡å…¨æ–‡", expanded=True):
                    for idx, article in enumerate(articles, 1):
                        st.markdown(f"**{idx}. {article['title']}**")
                        st.caption(f"PMC ID: {article['pmcid']} | å¹´ä»½: {article.get('year', 'N/A')} | {article.get('authors', 'N/A')[:50]}...")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**ğŸ§« ç»†èƒç³»:**")
                            if article.get('cell_lines'):
                                st.write(", ".join(article['cell_lines']))
                            else:
                                st.write("æœªæ˜ç¡®æåŠ")
                            
                            st.markdown("**ğŸ§¬ è´¨ç²’è½½ä½“:**")
                            if article.get('vectors'):
                                for v in article['vectors'][:5]:
                                    st.markdown(f"- `{v}`")
                            else:
                                st.write("æœªæå–åˆ°")
                        
                        with col2:
                            st.markdown("**ğŸ’Š ç­›é€‰æ¡ä»¶:**")
                            if article.get('selection'):
                                for sel in article['selection']:
                                    st.markdown(f"- {sel}")
                            else:
                                st.write("æœªæåŠ")
                        
                        if article.get('sequences'):
                            seq_data = article['sequences']
                            has_seqs = any(seq_data.values())
                            
                            if has_seqs:
                                st.markdown("**ğŸ¯ æå–çš„åºåˆ—:**")
                                seq_cols = st.columns(3)
                                
                                col_idx = 0
                                for seq_type, seq_list in seq_data.items():
                                    if seq_list and col_idx < 3:
                                        with seq_cols[col_idx]:
                                            st.markdown(f"*{seq_type.upper()}*")
                                            for s in seq_list[:3]:
                                                seq_text = s['sequence']
                                                gc = s.get('gc', 0)
                                                st.code(f"{seq_text}\nGC:{gc}%", language="text")
                                        col_idx += 1
                        
                        with st.expander("æŸ¥çœ‹MethodsåŸæ–‡ç‰‡æ®µ"):
                            st.text(article.get('methods_text', 'æ— å†…å®¹')[:1000])
                        
                        st.divider()
    
    # Tab 5: å®éªŒèµ„æº
    with tabs[4]:
        st.subheader("Addgene è´¨ç²’èµ„æº")
        plasmids = result["addgene_plasmids"]
        
        if plasmids:
            for p in plasmids:
                st.write(f"**[{p['plasmid_id']}]** {p['name']}")
                st.caption(f"[æŸ¥çœ‹è¯¦æƒ…]({p['url']})")
                st.divider()
        else:
            st.info("æœªæ‰¾åˆ°ç›¸å…³è´¨ç²’")

if __name__ == "__main__":
    main()
