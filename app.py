# -*- coding: utf-8 -*-
"""
æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V1
- 2000bpåŒ…è£…æé™ç‰ˆ + åŒåˆ†æ”¯æ–‡çŒ®æ£€ç´¢
- æ•´åˆHPAäººç±»è›‹ç™½è¡¨è¾¾æ•°æ®ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
- å¢å¼ºæ…¢ç—…æ¯’é£é™©è¯„ä¼°ï¼ˆåŠŸèƒ½+æ–‡çŒ®+åºåˆ—ï¼‰
"""

import streamlit as st
import pandas as pd
import requests
from Bio import Entrez
from datetime import datetime
import time
import re
from typing import Dict, List, Optional, Tuple
import json
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import urllib.parse
from collections import Counter
import hashlib
import openai
import os
import io
import zipfile

# ==================== é…ç½®ä¸åˆå§‹åŒ– ====================
st.set_page_config(
    page_title="æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V1",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å®‰å…¨å¯†é’¥é…ç½®
try:
    NCBI_EMAIL = st.secrets["NCBI_EMAIL"]
    NCBI_API_KEY = st.secrets.get("NCBI_API_KEY", "")
    APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
    BAILIAN_API_KEY = st.secrets.get("BAILIAN_API_KEY") or st.secrets.get("DASHSCOPE_API_KEY", "")
    AI_MODEL = st.secrets.get("AI_MODEL", "qwen-plus")
except Exception:
    st.error("âš ï¸ è¯·å…ˆé…ç½® Secretsï¼ˆNCBI_EMAIL ç­‰ï¼‰")
    st.stop()

# å¯†ç ä¿æŠ¤
if APP_PASSWORD:
    if 'authenticated' not in st.session_state:
        st.session_state.authenticated = False
    if not st.session_state.authenticated:
        pwd = st.text_input("ğŸ”’ è¯·è¾“å…¥è®¿é—®å¯†ç ", type="password")
        if pwd == APP_PASSWORD:
            st.session_state.authenticated = True
            st.rerun()
        elif pwd:
            st.error("å¯†ç é”™è¯¯")
        st.stop()

# åˆå§‹åŒ– session state
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'search_history' not in st.session_state:
    st.session_state.search_history = []

# ==================== Addgene çˆ¬å–æ¨¡å— ====================

class AddgeneScraper:
    """Addgene è´¨ç²’çˆ¬å–å™¨ - ç²¾ç®€ç‰ˆ"""
    
    def __init__(self):
        self.base_url = "https://www.addgene.org"
        self.ua = UserAgent()
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        })
    
    @st.cache_data(ttl=86400, show_spinner=False)
    def search_plasmids(_self, gene_symbol: str, max_results: int = 5) -> List[Dict]:
        """æœç´¢ Addgene è´¨ç²’"""
        try:
            query = urllib.parse.quote(f"{gene_symbol}")
            search_url = f"{_self.base_url}/search/?q={query}&type=plasmid"
            headers = {'User-Agent': _self.ua.random}
            time.sleep(1)
            
            response = _self.session.get(search_url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            plasmids = []
            
            result_items = (soup.find_all('article', class_='addgene-search-result') or 
                           soup.find_all('div', class_='search-result-item') or
                           soup.select('.plasmid-item'))
            
            if not result_items:
                result_items = soup.select('[data-testid="plasmid-card"]')
            
            for item in result_items[:max_results]:
                try:
                    plasmid_data = _self._parse_plasmid_card(item, gene_symbol)
                    if plasmid_data:
                        plasmids.append(plasmid_data)
                except Exception:
                    continue
            
            if not plasmids:
                plasmids = _self._search_by_gene_page(gene_symbol)
            
            return plasmids
            
        except Exception as e:
            st.error(f"Addgene çˆ¬å–é”™è¯¯: {str(e)}")
            return []
    
    def _parse_plasmid_card(self, card, gene_symbol: str) -> Optional[Dict]:
        """è§£æå•ä¸ªè´¨ç²’å¡ç‰‡"""
        try:
            link_tag = card.find('a', href=re.compile(r'/\d{5,6}/')) or card.find('a', href=True)
            if not link_tag:
                return None
            
            href = link_tag.get('href', '')
            plasmid_id_match = re.search(r'/(\d{5,6})/', href)
            if not plasmid_id_match:
                return None
            
            plasmid_id = plasmid_id_match.group(1)
            full_url = f"{self.base_url}{href}" if href.startswith('/') else href
            
            name_tag = card.find('h3') or card.find('h2') or card.find('a', class_='title')
            name = name_tag.get_text(strip=True) if name_tag else "Unknown"
            
            name_lower = name.lower()
            desc = card.get_text().lower()
            gene_lower = gene_symbol.lower()
            
            if gene_lower not in name_lower and gene_lower not in desc:
                return None
            
            return {
                'plasmid_id': plasmid_id,
                'name': name,
                'url': full_url,
                'insert_gene': gene_symbol,
            }
            
        except Exception:
            return None
    
    def _search_by_gene_page(self, gene_symbol: str) -> List[Dict]:
        """é€šè¿‡åŸºå› ä¸“å±é¡µé¢æœç´¢"""
        try:
            gene_url = f"{self.base_url}/browse/gene/{gene_symbol}/"
            headers = {'User-Agent': self.ua.random}
            response = self.session.get(gene_url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'lxml')
            plasmids = []
            seen_ids = set()
            
            links = soup.find_all('a', href=re.compile(r'/\d{5,6}/'))
            for link in links[:5]:
                href = link.get('href', '')
                match = re.search(r'/(\d{5,6})/', href)
                if match:
                    pid = match.group(1)
                    if pid not in seen_ids:
                        seen_ids.add(pid)
                        name = link.get_text(strip=True) or f"{gene_symbol} related"
                        
                        if gene_symbol.lower() not in name.lower():
                            continue
                            
                        plasmids.append({
                            'plasmid_id': pid,
                            'name': name,
                            'url': f"{self.base_url}/{pid}/",
                            'insert_gene': gene_symbol,
                            'source': 'Gene page'
                        })
            return plasmids
        except Exception:
            return []


# ==================== HPA åŸºå› è¡¨è¾¾æ•°æ®æ¨¡å— ====================

class HPAGeneData:
    """åŸºäºæœ¬åœ° proteinatlas.tsv çš„äººç±»è›‹ç™½è¡¨è¾¾æ•°æ®æŸ¥è¯¢"""
    
    def __init__(self, tsv_path: str = "data/proteinatlas.tsv"):
        self.tsv_path = tsv_path
        self.df = None
        self.available_columns = []
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½ TSV æ–‡ä»¶ï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ï¼‰"""
        try:
            if not os.path.exists(self.tsv_path):
                self._auto_download()
                if not os.path.exists(self.tsv_path):
                    return
            
            self.df = pd.read_csv(self.tsv_path, sep='\t', low_memory=False)
            self.available_columns = self.df.columns.tolist()
            self.df.columns = [col.strip() for col in self.df.columns]
            
            if 'Gene' not in self.df.columns:
                gene_col = None
                for col in self.df.columns:
                    if 'gene' in col.lower():
                        gene_col = col
                        break
                if gene_col:
                    self.df = self.df.rename(columns={gene_col: 'Gene'})
                else:
                    st.error("HPA æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åŸºå› ååˆ—")
                    self.df = pd.DataFrame()
                    return
            
            st.success(f"âœ… HPA æ•°æ®å·²åŠ è½½: {len(self.df):,} æ¡åŸºå› ")
            
        except Exception as e:
            st.error(f"åŠ è½½ HPA æ•°æ®å¤±è´¥: {e}")
            self.df = pd.DataFrame()
    
    def _auto_download(self):
        """è‡ªåŠ¨ä¸‹è½½ HPA æ•°æ®æ–‡ä»¶"""
        try:
            st.info("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ HPA æ•°æ®...")
            os.makedirs("data", exist_ok=True)
            
            url = "https://www.proteinatlas.org/download/proteinatlas.tsv.zip"
            response = requests.get(url, timeout=300)
            
            with open("data/hpa_temp.zip", "wb") as f:
                f.write(response.content)
            
            with zipfile.ZipFile("data/hpa_temp.zip", 'r') as zip_ref:
                zip_ref.extractall("data")
            
            os.remove("data/hpa_temp.zip")
            st.success("âœ… HPA æ•°æ®ä¸‹è½½å®Œæˆ")
            
        except Exception as e:
            st.error(f"ä¸‹è½½å¤±è´¥: {e}")
    
    def get_gene_data(self, gene_symbol: str) -> Dict:
        """è·å–åŸºå› æ•°æ®"""
        if self.df is None or self.df.empty:
            return {}
        
        try:
            mask = self.df['Gene'].str.upper() == gene_symbol.upper()
            if not mask.any():
                return {}
            
            row = self.df[mask].iloc[0]
            result = {}
            
            if 'Ensembl' in self.df.columns:
                result['ensembl_id'] = str(row['Ensembl'])
            if 'Uniprot' in self.df.columns:
                result['uniprot_id'] = str(row['Uniprot'])
            if 'Subcellular main location' in self.df.columns:
                result['subcellular_location'] = str(row['Subcellular main location'])
            elif 'Subcellular location' in self.df.columns:
                result['subcellular_location'] = str(row['Subcellular location'])
            
            rna_col = None
            for col in self.df.columns:
                if 'tissue' in col.lower() and 'rna' in col.lower():
                    rna_col = col
                    break
            if rna_col:
                result['rna_tissue_specificity'] = str(row[rna_col])
            
            if 'Reliability' in self.df.columns:
                result['reliability'] = str(row['Reliability'])
            else:
                result['reliability'] = 'N/A'
            
            if 'ensembl_id' in result:
                result['hpa_link'] = f"https://www.proteinatlas.org/{result['ensembl_id']}"
            
            return result
            
        except Exception as e:
            st.error(f"æŸ¥è¯¢ HPA æ•°æ®é”™è¯¯: {e}")
            return {}
    
    def check_data_available(self) -> bool:
        return self.df is not None and not self.df.empty


# ==================== æ…¢ç—…æ¯’é£é™©è¯„ä¼°ç±» ====================

class LentiviralRiskAssessor:
    """æ…¢ç—…æ¯’åŒ…è£…é£é™©è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.risk_keywords = {
            "high": {
                "antiviral": ["interferon", "ifn", "antiviral", "innate immunity", "rig-i", "tlr", "sting", "mavs", "irf"],
                "toxic": ["lethal", "essential", "cell death", "apoptosis", "toxic", "fatal"],
                "proliferation": ["cell cycle arrest", "growth inhibition", "anti-proliferative", "tumor suppressor", "contact inhibition"],
                "structure": ["transmembrane domain", "secreted protein", "extracellular matrix", "collagen"]
            },
            "medium": {
                "signaling": ["kinase", "phosphatase", "signal transduction", "pathway"],
                "transcription": ["transcription factor", "nuclear receptor", "epigenetic"]
            }
        }
    
    def assess_by_function(self, gene_description: str, phenotype: str) -> Dict:
        """æ ¹æ®åŸºå› åŠŸèƒ½è¯„ä¼°é£é™©"""
        desc_lower = gene_description.lower()
        risks = []
        risk_level = "low"
        
        for level, categories in self.risk_keywords.items():
            for category, keywords in categories.items():
                matched = [kw for kw in keywords if kw in desc_lower]
                if matched:
                    risks.append(f"{category}: æ£€æµ‹åˆ°å…³é”®è¯ {matched[:2]}...")
                    if level == "high":
                        risk_level = "high"
                    elif level == "medium" and risk_level != "high":
                        risk_level = "medium"
        
        if "å¿…éœ€" in phenotype or "lethal" in phenotype.lower() or "essential" in phenotype.lower():
            risks.append("è‡´æ­»æ€§: å¿…éœ€åŸºå› ï¼Œæ•²é™¤å¯èƒ½å¯¼è‡´ç»†èƒæ­»äº¡")
            risk_level = "high"
        
        return {
            "risk_level": risk_level,
            "risks": risks,
            "recommendation": self._get_recommendation(risk_level, risks)
        }
    
    def _get_recommendation(self, risk_level: str, risks: List[str]) -> str:
        """æ ¹æ®é£é™©ç­‰çº§ç»™å‡ºå»ºè®®"""
        if risk_level == "high":
            return "âš ï¸ é«˜é£é™©ï¼šå»ºè®®ä½¿ç”¨è¯±å¯¼å‹ç³»ç»Ÿ(Tet-On/Off)æˆ–æš‚æ—¶æ€§æ•²ä½(shRNA)ï¼Œé¿å…ç›´æ¥KO"
        elif risk_level == "medium":
            return "âš¡ ä¸­ç­‰é£é™©ï¼šå¯è¿›è¡ŒKOä½†éœ€å¯†åˆ‡ç›‘æµ‹ç»†èƒçŠ¶æ€ï¼Œå»ºè®®å‡†å¤‡è¯±å¯¼å‹å¤‡é€‰æ–¹æ¡ˆ"
        else:
            return "âœ… ä½é£é™©ï¼šæ ‡å‡†KOæ–¹æ¡ˆé€‚ç”¨ï¼Œé¢„æœŸå¯è·å¾—ç¨³å®šç»†èƒç³»"
    
    def assess_by_literature(self, literature_data: Dict) -> Dict:
        """æ ¹æ®æ–‡çŒ®è¯„ä¼°åŒ…è£…å¯è¡Œæ€§"""
        oe_count = literature_data.get("overexpression", {}).get("count", 0)
        kd_count = literature_data.get("knockdown", {}).get("count", 0)
        ko_count = literature_data.get("knockout", {}).get("count", 0)
        
        evidence = {
            "overexpression": {"available": oe_count > 0, "count": oe_count, "method": "æ…¢ç—…æ¯’åŒ…è£…"},
            "knockdown": {"available": kd_count > 0, "count": kd_count, "method": "shRNA/siRNA"},
            "knockout": {"available": ko_count > 0, "count": ko_count, "method": "CRISPR/Cas9"}
        }
        
        if oe_count > 10:
            packaging_feasibility = "high"
        elif oe_count > 0:
            packaging_feasibility = "medium"
        else:
            packaging_feasibility = "unknown"
        
        return {
            "evidence": evidence,
            "packaging_feasibility": packaging_feasibility,
            "has_precedent": oe_count > 0 or kd_count > 0 or ko_count > 0
        }
    
    def extract_sequences_from_literature(self, articles: List[Dict]) -> Dict:
        """ä»æ–‡çŒ®ä¸­æå–shRNA/siRNA/sgRNAåºåˆ—"""
        sequences = {
            "shrna": [],
            "sirna": [],
            "sgrna": []
        }
        
        patterns = {
            "shrna": r'(?:shRNA|shRNA\s+sequence)[:\s]+([ACGTU]{19,23})',
            "sirna": r'(?:siRNA|siRNA\s+sequence)[:\s]+([ACGTU]{19,23})',
            "sgrna": r'(?:sgRNA|gRNA|guide\s+RNA)[:\s]+([ACGTU]{20,23})',
            "target_seq": r'target\s+sequence[:\s]+([ACGTU]{19,23})',
            "forward": r'[Ff]orward[:\s]+([ACGTU]{19,23})',
            "sense": r'[Ss]ense[:\s]+([ACGTU]{19,23})'
        }
        
        for article in articles:
            text = article.get("title", "") + " " + article.get("abstract_snippet", "")
            
            for seq_type, pattern in patterns.items():
                matches = re.findall(pattern, text)
                for match in matches:
                    seq = match[0] if isinstance(match, tuple) else match
                    seq = seq.upper().replace("U", "T")
                    
                    if len(seq) >= 19 and len(seq) <= 23 and all(c in "ATCG" for c in seq):
                        entry = {
                            "sequence": seq,
                            "pmid": article.get("pmid"),
                            "title": article.get("title", "")[:50] + "..." if len(article.get("title", "")) > 50 else article.get("title", ""),
                            "type": seq_type
                        }
                        
                        if "shrna" in seq_type.lower():
                            if not any(e["sequence"] == seq for e in sequences["shrna"]):
                                sequences["shrna"].append(entry)
                        elif "sirna" in seq_type.lower():
                            if not any(e["sequence"] == seq for e in sequences["sirna"]):
                                sequences["sirna"].append(entry)
                        elif "grna" in seq_type.lower() or "sgrna" in seq_type.lower():
                            if not any(e["sequence"] == seq for e in sequences["sgrna"]):
                                sequences["sgrna"].append(entry)
        
        for seq_type in sequences:
            sequences[seq_type] = sequences[seq_type][:5]
        
        return sequences


# ==================== NCBI æ•°æ®è·å–æ¨¡å— ====================

class BioDataFetcher:
    def __init__(self, email: str, api_key: str = ""):
        Entrez.email = email
        if api_key:
            Entrez.api_key = api_key
        self.uniprot_base = "https://rest.uniprot.org/uniprotkb/search.json"
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        self.addgene_scraper = AddgeneScraper()
        self.hpa_data = HPAGeneData()
    
    def get_ncbi_gene_info(self, gene_symbol: str, species: str) -> Dict:
        try:
            term = f"{gene_symbol}[Gene Name] AND {species}[Organism]"
            handle = Entrez.esearch(db="gene", term=term, retmax=1)
            record = Entrez.read(handle)
            handle.close()
            
            if not record["IdList"]:
                return {"status": "not_found", "error": f"æœªæ‰¾åˆ° {gene_symbol} ({species})"}
            
            gene_id = record["IdList"][0]
            handle = Entrez.efetch(db="gene", id=gene_id, rettype="xml")
            gene_data = Entrez.read(handle)
            handle.close()
            
            gene_entry = gene_data[0]
            summary = gene_entry.get("Entrezgene_summary", "")
            
            lethal_keywords = ["essential", "lethal", "required for cell viability", 
                             "knockout mice die", "embryonic lethal"]
            phenotype = "éå¿…éœ€"
            if any(kw in summary.lower() for kw in lethal_keywords):
                phenotype = "å¿…éœ€ï¼ˆæ½œåœ¨è‡´æ­»é£é™©ï¼‰"
            
            return {
                "gene_id": gene_id,
                "symbol": gene_symbol,
                "species": species,
                "description": summary[:800] if summary else "æ— æè¿°",
                "phenotype": phenotype,
                "chromosome": gene_entry.get("Entrezgene_location", [{}])[0].get("Gene-location", {}).get("Gene-location_chromosome", "N/A"),
                "status": "success"
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def get_uniprot_info(self, gene_symbol: str, species: str) -> Dict:
        """å¢å¼ºç‰ˆ UniProt æŸ¥è¯¢ï¼Œæ”¯æŒå¤šç§åŒ¹é…ç­–ç•¥"""
        try:
            species_map = {
                "Homo sapiens": ("human", 9606),
                "Mus musculus": ("mouse", 10090),
                "Rattus norvegicus": ("rat", 10116)
            }
            org_name, tax_id = species_map.get(species, (species.lower(), None))
            
            queries = [
                f"gene:{gene_symbol}+organism_id:{tax_id}",
                f"gene:{gene_symbol}+organism:{org_name}",
                f"{gene_symbol}+organism:{org_name}",
                gene_symbol
            ]
            
            for query in queries:
                try:
                    params = {
                        "query": query,
                        "fields": "accession,gene_names,length,cc_subcellular_location,sequence,protein_name",
                        "format": "json",
                        "size": 5
                    }
                    
                    response = requests.get(
                        self.uniprot_base, 
                        params=params, 
                        headers=self.headers, 
                        timeout=15
                    )
                    data = response.json()
                    
                    if data.get("results"):
                        best_match = None
                        gene_names = []
                        
                        for protein in data["results"]:
                            genes = protein.get("genes", [])
                            for g in genes:
                                if g.get("geneName"):
                                    gene_names.append(g["geneName"].get("value", "").upper())
                                if g.get("synonyms"):
                                    for syn in g["synonyms"]:
                                        gene_names.append(syn.get("value", "").upper())
                            
                            if gene_symbol.upper() in gene_names:
                                best_match = protein
                                break
                        
                        if not best_match:
                            best_match = data["results"][0]
                        
                        accession = best_match.get("primaryAccession", "")
                        seq_length = best_match.get("sequence", {}).get("length", 0)
                        
                        loc_text = ""
                        comments = best_match.get("comments", [])
                        for comment in comments:
                            if comment.get("commentType") == "SUBCELLULAR LOCATION":
                                locations = comment.get("subcellularLocations", [])
                                locs = [loc.get("location", {}).get("value", "") 
                                       for loc in locations if loc.get("location")]
                                loc_text = "; ".join([l for l in locs if l])
                        
                        cds_length = seq_length * 3 if seq_length else 0
                        
                        return {
                            "uniprot_id": accession,
                            "protein_length": seq_length,
                            "cds_length_bp": cds_length,
                            "subcellular_location": loc_text or "æœªæ ‡æ³¨",
                            "protein_name": best_match.get("proteinDescription", {}).get("recommendedName", {}).get("fullName", {}).get("value", ""),
                            "status": "success",
                            "match_type": "exact" if gene_symbol.upper() in gene_names else "partial"
                        }
                except:
                    continue
            
            return {
                "status": "not_found",
                "error": f"UniProt æœªæ‰¾åˆ° {gene_symbol}ï¼Œè¯·ç¡®è®¤åŸºå› ç¬¦å·"
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)}


# ==================== é€šä¹‰åƒé—® AI åˆ†ææ¨¡å— ====================

class AIAnalyzer:
    def __init__(self):
        self.client = None
        self.model = None
        self.cache = {}
        
        try:
            api_key = BAILIAN_API_KEY
            
            if not api_key:
                raise ValueError("æœªé…ç½® BAILIAN_API_KEY")
            
            self.model = AI_MODEL
            
            self.client = openai.OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            
            st.success(f"âœ… é€šä¹‰åƒé—®å·²è¿æ¥ï¼ˆæ¨¡å‹ï¼š{self.model}ï¼‰")
            
        except Exception as e:
            st.error(f"é€šä¹‰åƒé—®åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            raise e
    
    def _get_cache_key(self, text: str, task: str) -> str:
        return hashlib.md5(f"{task}:{text}".encode()).hexdigest()[:16]
    
    def _call_qwen(self, prompt: str, json_mode: bool = True, temperature: float = 0.3) -> Dict:
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯èµ„æ·±ç»†èƒç”Ÿç‰©å­¦å’Œåˆ†å­å…‹éš†ä¸“å®¶ï¼Œæ“…é•¿åˆ†æåŸºå› åŠŸèƒ½ã€è®¾è®¡ç»†èƒç³»æ„å»ºå®éªŒæ–¹æ¡ˆã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸“ä¸šä¸”å…·ä½“ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": 2000
            }
            
            if json_mode:
                kwargs["response_format"] = {"type": "json_object"}
            
            with st.spinner("ğŸ¤– é€šä¹‰åƒé—®åˆ†æä¸­..."):
                response = self.client.chat.completions.create(**kwargs)
                content = response.choices[0].message.content
                
                if json_mode:
                    return json.loads(content)
                return {"result": content}
                
        except Exception as e:
            st.error(f"é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}")
            return {"error": str(e)}
    
    def assess_gene_essentiality(self, gene_data: Dict, uniprot_data: Dict) -> Dict:
        gene_name = gene_data.get("symbol", "")
        description = gene_data.get("description", "")
        location = uniprot_data.get("subcellular_location", "")
        
        cache_key = self._get_cache_key(f"{gene_name}_{description[:100]}", "essentiality")
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹åŸºå› ä¿¡æ¯ï¼Œåˆ†æè¯¥åŸºå› æ˜¯å¦ä¸ºç»†èƒå¿…éœ€åŸºå› ï¼ˆessential geneï¼‰ï¼Œå¹¶è¯„ä¼°æ„å»ºKOç»†èƒç³»çš„å¯è¡Œæ€§ã€‚

åŸºå› åç§°ï¼š{gene_name}
åŸºå› åŠŸèƒ½æè¿°ï¼š{description}
äºšç»†èƒå®šä½ï¼š{location}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_essential": "æ˜¯/å¦/å¯èƒ½",
    "confidence": "é«˜/ä¸­/ä½",
    "rationale": "è¯¦ç»†è§£é‡Šåˆ¤æ–­ç†ç”±ï¼ŒåŸºäºè¯¥åŸºå› çš„ç”Ÿç‰©å­¦åŠŸèƒ½",
    "expected_phenotype": "å¦‚æœæ•²é™¤ï¼Œé¢„æœŸä¼šå‡ºç°ä»€ä¹ˆç»†èƒè¡¨å‹ï¼ˆå¦‚ç»†èƒæ­»äº¡ã€å¢æ®–å‡æ…¢ã€å‘¨æœŸé˜»æ»ç­‰ï¼‰",
    "construction_difficulty": "å®¹æ˜“/ä¸­ç­‰/å›°éš¾",
    "recommendation": "å…·ä½“çš„å®éªŒå»ºè®®ï¼ˆå¦‚æ˜¯å¦å»ºè®®ä½¿ç”¨è¯±å¯¼å‹ç³»ç»Ÿã€å…ˆå°è¯•KDè¿˜æ˜¯ç›´æ¥KOï¼‰",
    "key_considerations": "å®éªŒè®¾è®¡ä¸­çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼ˆ2-3ç‚¹ï¼‰",
    "alternative_approaches": "å¦‚æœKOå›°éš¾ï¼Œå»ºè®®çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚ä½¿ç”¨siRNAã€è¯±å¯¼å‹æ•²é™¤ç­‰ï¼‰"
}}

æ³¨æ„ï¼š
1. å¦‚æœåŸºå› æ˜¯ç®¡å®¶åŸºå› ï¼ˆå¦‚GAPDHã€ACTBï¼‰æˆ–å‚ä¸DNAå¤åˆ¶/ä¿®å¤æ ¸å¿ƒåŠŸèƒ½ï¼Œé€šå¸¸åˆ¤å®šä¸ºå¿…éœ€
2. å¦‚æœæ˜¯ç™ŒåŸºå› æˆ–ä¿¡å·é€šè·¯åˆ†å­ï¼Œé€šå¸¸ä¸ºéå¿…éœ€ä½†å¯èƒ½æœ‰è¡¨å‹
3. è¯·ç»™å‡ºå…·ä½“ç”Ÿç‰©å­¦æœºåˆ¶è§£é‡Šï¼Œä¸è¦æ³›æ³›è€Œè°ˆ"""
        
        result = self._call_qwen(prompt, json_mode=True)
        self.cache[cache_key] = result
        return result
    
    def analyze_literature_deep(self, articles: List[Dict], gene: str, construct_type: str) -> Dict:
        if not articles:
            return {"summary": "æ— ç›¸å…³æ–‡çŒ®", "protocols": []}
        
        articles_text = "\n\n".join([
            f"æ–‡çŒ®{i+1}:\næ ‡é¢˜ï¼š{a['title']}\næ–¹æ³•å…³é”®è¯ï¼š{a['methods']}\næ‘˜è¦ç‰‡æ®µï¼š{a.get('abstract_snippet', '')}"
            for i, a in enumerate(articles[:5])
        ])
        
        cache_key = self._get_cache_key(articles_text[:500], f"lit_{construct_type}")
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        prompt = f"""ä½œä¸ºæ–¹æ³•å­¦ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹å…³äº"{gene}"åŸºå› {construct_type}ï¼ˆè¿‡è¡¨è¾¾/æ•²ä½/æ•²é™¤ï¼‰ç»†èƒç³»æ„å»ºçš„æ–‡çŒ®ã€‚

{articles_text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "summary": "è¿™äº›ç ”ç©¶çš„æ€»ä½“æ–¹æ³•å­¦è¶‹åŠ¿ï¼ˆä¸­æ–‡ï¼Œ2-3å¥è¯æ¦‚æ‹¬ï¼‰",
    "protocols": [
        {{
            "cell_line": "ä½¿ç”¨çš„ç»†èƒç³»",
            "vector_system": "å…·ä½“è½½ä½“åç§°ï¼ˆå¦‚pLKO.1, lentiCRISPRv2, pLVXç­‰ï¼‰",
            "delivery_method": "é€’é€æ–¹æ³•ï¼ˆå¦‚æ…¢ç—…æ¯’è½¬å¯¼ã€è„‚è´¨ä½“è½¬æŸ“Lipofectamine 2000/3000ã€ç”µè½¬ç­‰ï¼‰",
            "selection_method": "ç­›é€‰æ–¹æ³•å’Œæµ“åº¦ï¼ˆå¦‚Puromycin 2Î¼g/mL, G418 500Î¼g/mLç­‰ï¼‰",
            "validation_method": "éªŒè¯æ–¹æ³•ï¼ˆå¦‚Western Blotã€qPCRã€å…ç–«è§å…‰ã€æµå¼ç­‰ï¼‰",
            "efficiency": "æ•ˆç‡ä¿¡æ¯ï¼ˆå¦‚æœ‰æåŠï¼‰",
            "duration": "å®éªŒå‘¨æœŸï¼ˆå¦‚æœ‰æåŠï¼‰"
        }}
    ],
    "common_methods": "æœ€å¸¸è§çš„æ–¹æ³•ç»„åˆï¼ˆä¸­æ–‡æ€»ç»“ï¼‰",
    "critical_factors": "å½±å“å®éªŒæˆåŠŸçš„å…³é”®å› ç´ ï¼ˆ2-3ç‚¹ï¼‰",
    "troubleshooting": "å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ±¡æŸ“ã€æ•ˆç‡ä½ã€ç»†èƒæ­»äº¡ç­‰ï¼‰",
    "recommendation": "é’ˆå¯¹è¯¥åŸºå› {construct_type}çš„å…·ä½“æ“ä½œå»ºè®®"
}}

æ³¨æ„ï¼š
1. å¦‚æœæŸå­—æ®µåœ¨æ–‡çŒ®ä¸­æœªæåŠï¼Œå¡«å†™"æœªæåŠ"
2. ä¼˜å…ˆæå–å…·ä½“çš„è¯•å‰‚åç§°å’Œæµ“åº¦ï¼ˆå¦‚Lipofectamine 2000, Polybrene 8Î¼g/mLç­‰ï¼‰
3. å¦‚æœæåˆ°å¤šç§ç»†èƒç³»ï¼Œåˆ—å‡ºæœ€å¸¸ç”¨çš„2-3ç§"""
        
        result = self._call_qwen(prompt, json_mode=True)
        self.cache[cache_key] = result
        return result


# ==================== åˆ†æä¸»ç±» ====================

class ConstructAnalyzer:
    def __init__(self):
        self.fetcher = BioDataFetcher(NCBI_EMAIL, NCBI_API_KEY)
        self.risk_assessor = LentiviralRiskAssessor()
    
    def analyze_gene(self, gene_symbol: str, species: str, 
                    cell_line: Optional[str] = None, 
                    cell_species: Optional[str] = None) -> Dict:
        
        with st.spinner(f"ğŸ” æ­£åœ¨æ·±åº¦åˆ†æ {gene_symbol}..."):
            
            st.text("æ£€ç´¢ NCBI Gene...")
            ncbi_info = self.fetcher.get_ncbi_gene_info(gene_symbol, species)
            time.sleep(0.5)
            
            st.text("æ£€ç´¢ UniProt...")
            uniprot_info = self.fetcher.get_uniprot_info(gene_symbol, species)
            time.sleep(0.5)
            
            st.text("æ£€ç´¢ Addgene...")
            addgene_plasmids = self.fetcher.addgene_scraper.search_plasmids(gene_symbol)
            time.sleep(0.5)
            
            hpa_gene_data = {}
            if species == "Homo sapiens":
                st.text("æ£€ç´¢ HPA è›‹ç™½è¡¨è¾¾æ•°æ®...")
                hpa_gene_data = self.fetcher.hpa_data.get_gene_data(gene_symbol)
            time.sleep(0.5)
            
            st.text("æ£€ç´¢æ–‡çŒ®...")
            literature = self._search_all_constructs(gene_symbol, cell_line)
            time.sleep(0.5)
            
            st.text("è¯„ä¼°æ…¢ç—…æ¯’é£é™©...")
            lentiviral = self._assess_lentiviral_comprehensive(
                gene_symbol, ncbi_info, uniprot_info, literature
            )
            
            ai_analysis = {}
            try:
                ai_analyzer = AIAnalyzer()
                
                with st.spinner("ğŸ¤– é€šä¹‰åƒé—®æ­£åœ¨æ·±åº¦åˆ†æ..."):
                    ai_analysis["gene_assessment"] = ai_analyzer.assess_gene_essentiality(
                        ncbi_info, uniprot_info
                    )
                    
                    if cell_line and literature.get("specific_cell", {}).get("found"):
                        for ctype in ["overexpression", "knockdown", "knockout"]:
                            if literature["specific_cell"][ctype]["articles"]:
                                ai_result = ai_analyzer.analyze_literature_deep(
                                    literature["specific_cell"][ctype]["articles"], gene_symbol, ctype
                                )
                                literature["specific_cell"][ctype]["ai_analysis"] = ai_result
                    
                    for ctype in ["overexpression", "knockdown", "knockout"]:
                        if literature[ctype]["articles"]:
                            ai_result = ai_analyzer.analyze_literature_deep(
                                literature[ctype]["articles"], gene_symbol, ctype
                            )
                            literature[ctype]["ai_analysis"] = ai_result
                    
                    ai_analysis["enabled"] = True
                    
            except Exception as e:
                st.warning(f"AIåˆ†ææœªå¯ç”¨ï¼š{e}")
                ai_analysis["enabled"] = False
            
            result = {
                "input_info": {
                    "gene_symbol": gene_symbol,
                    "species": species,
                    "cell_line": cell_line or "æœªæŒ‡å®š",
                    "cell_species": cell_species or "æœªæŒ‡å®š",
                    "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
                    "ai_enabled": ai_analysis.get("enabled", False)
                },
                "gene_function": ncbi_info,
                "protein_data": uniprot_info,
                "hpa_gene_data": hpa_gene_data,
                "addgene_plasmids": addgene_plasmids,
                "lentiviral_assessment": lentiviral,
                "cell_line_constructs": literature,
                "ai_analysis": ai_analysis,
                "database_record": self._format_database_record(
                    gene_symbol, species, cell_line, ncbi_info, uniprot_info, 
                    lentiviral, literature, addgene_plasmids, hpa_gene_data
                )
            }
            
            return result
    
    def _assess_lentiviral_comprehensive(self, gene_symbol: str, ncbi_info: Dict, 
                                        uniprot_info: Dict, literature: Dict) -> Dict:
        """ç»¼åˆè¯„ä¼°æ…¢ç—…æ¯’é€‚ç”¨æ€§"""
        cds_len = uniprot_info.get("cds_length_bp", 0)
        
        # 1. CDSé•¿åº¦è¯„ä¼°
        if cds_len > 2500:
            cds_risk = {"level": "high", "suitable": False, "reason": f"CDSè¿‡é•¿({cds_len}bp)ï¼Œè¿œè¶…2000bpæ¨èå€¼"}
        elif cds_len > 2000:
            cds_risk = {"level": "medium", "suitable": True, "reason": f"CDSè¾ƒé•¿({cds_len}bp)ï¼Œå»ºè®®ä½¿ç”¨ç¬¬ä¸‰ä»£ç³»ç»Ÿ"}
        elif cds_len == 0:
            cds_risk = {"level": "unknown", "suitable": True, "reason": "æ— æ³•è·å–CDSé•¿åº¦ä¿¡æ¯"}
        else:
            cds_risk = {"level": "low", "suitable": True, "reason": f"CDSé•¿åº¦åˆé€‚({cds_len}bp)"}
        
        # 2. åŸºå› åŠŸèƒ½é£é™©è¯„ä¼°
        function_risk = self.risk_assessor.assess_by_function(
            ncbi_info.get("description", ""),
            ncbi_info.get("phenotype", "")
        )
        
        # 3. æ–‡çŒ®è¯æ®è¯„ä¼°
        lit_evidence = self.risk_assessor.assess_by_literature(literature)
        
        # 4. æå–åºåˆ—
        sequences = {}
        for ctype in ["knockdown", "knockout"]:
            if literature[ctype].get("articles"):
                seqs = self.risk_assessor.extract_sequences_from_literature(
                    literature[ctype]["articles"]
                )
                if any(seqs.values()):
                    sequences[ctype] = seqs
        
        # ç»¼åˆå»ºè®®
        recommendations = []
        warnings = []
        
        if cds_risk["level"] == "high":
            warnings.append(f"âš ï¸ é•¿åº¦é£é™©: {cds_risk['reason']}")
            recommendations.append("å»ºè®®ä½¿ç”¨split-vectorç³»ç»Ÿæˆ–é€‰æ‹©å…¶ä»–é€’é€æ–¹å¼ï¼ˆå¦‚è½¬åº§å­ï¼‰")
        elif cds_risk["level"] == "medium":
            warnings.append(f"âš¡ é•¿åº¦è­¦å‘Š: {cds_risk['reason']}")
        
        if function_risk["risk_level"] == "high":
            warnings.append(f"ğŸš¨ åŠŸèƒ½é£é™©: {', '.join(function_risk['risks'][:2])}")
            recommendations.append("å¼ºçƒˆå»ºè®®ä½¿ç”¨è¯±å¯¼å‹è¡¨è¾¾ç³»ç»Ÿï¼ˆTet-On/Offï¼‰")
        elif function_risk["risk_level"] == "medium":
            warnings.append(f"âš¡ åŠŸèƒ½æ³¨æ„: {', '.join(function_risk['risks'][:1])}")
        
        if not lit_evidence["has_precedent"]:
            warnings.append("ğŸ“š æ–‡çŒ®ç¼ºä¹: æœªæ‰¾åˆ°è¯¥åŸºå› çš„ç—…æ¯’åŒ…è£…æ–‡çŒ®è®°å½•")
            recommendations.append("å»ºè®®å…ˆè¿›è¡Œå°è§„æ¨¡åŒ…è£…æµ‹è¯•")
        else:
            if lit_evidence["evidence"]["overexpression"]["available"]:
                recommendations.append("âœ… æ–‡çŒ®æ”¯æŒ: å·²æœ‰æˆåŠŸè¿‡è¡¨è¾¾è®°å½•")
        
        # æ€»ä½“è¯„çº§
        if cds_risk["level"] == "high" or function_risk["risk_level"] == "high":
            overall_rating = "âŒ é«˜é£é™©ï¼ˆä¸æ¨èæ ‡å‡†æ–¹æ¡ˆï¼‰"
        elif cds_risk["level"] == "medium" or function_risk["risk_level"] == "medium":
            overall_rating = "âš ï¸ ä¸­ç­‰é£é™©ï¼ˆéœ€ä¼˜åŒ–æ–¹æ¡ˆï¼‰"
        else:
            overall_rating = "âœ… ä½é£é™©ï¼ˆæ ‡å‡†æ–¹æ¡ˆé€‚ç”¨ï¼‰"
        
        return {
            "cds_assessment": cds_risk,
            "function_risk": function_risk,
            "literature_evidence": lit_evidence,
            "sequences": sequences,
            "warnings": warnings,
            "recommendations": recommendations,
            "overall_rating": overall_rating,
            "overall_suitable": cds_risk["suitable"] and function_risk["risk_level"] != "high"
        }
    
    def _search_all_constructs(self, gene_symbol: str, cell_line: Optional[str]) -> Dict:
        """åŒåˆ†æ”¯æ–‡çŒ®æ£€ç´¢"""
        results = {}
        
        # åˆ†æ”¯1ï¼šç‰¹å®šç»†èƒç³»
        if cell_line:
            query_specific = f'{gene_symbol}[Title/Abstract] AND {cell_line}[Title/Abstract] AND (cell line OR cell-line)'
            
            try:
                handle = Entrez.esearch(db="pubmed", term=query_specific, retmax=100, sort="relevance")
                record = Entrez.read(handle)
                handle.close()
                
                pmids = record["IdList"]
                
                if pmids:
                    fetch_ids = pmids[:20]
                    handle = Entrez.efetch(db="pubmed", id=fetch_ids, rettype="abstract", retmode="xml")
                    articles = Entrez.read(handle)
                    handle.close()
                    
                    specific_oe, specific_kd, specific_ko = [], [], []
                    
                    for article in articles.get("PubmedArticle", []):
                        try:
                            medline = article["MedlineCitation"]
                            article_data = medline["Article"]
                            title = article_data.get("ArticleTitle", "").lower()
                            abstract = str(article_data.get("Abstract", {}).get("AbstractText", "")).lower()
                            full_text = title + " " + abstract
                            
                            pmid = str(medline.get("PMID", "N/A"))
                            title_display = article_data.get("ArticleTitle", "N/A")
                            
                            methods = [kw for kw in ["lentiviral", "transfection", "electroporation", "transduction"] if kw in full_text]
                            
                            article_info = {
                                "pmid": pmid,
                                "title": title_display,
                                "methods": ", ".join(methods) if methods else "æœªæ˜ç¡®æåŠ",
                                "cell_line": cell_line,
                                "abstract_snippet": abstract[:300] if len(abstract) > 300 else abstract
                            }
                            
                            if any(kw in full_text for kw in ["overexpression", "over-expression", "ectopic"]):
                                specific_oe.append(article_info)
                            elif any(kw in full_text for kw in ["knockdown", "sirna", "shrna"]):
                                specific_kd.append(article_info)
                            elif any(kw in full_text for kw in ["knockout", "crispr", "knock-out"]):
                                specific_ko.append(article_info)
                                    
                        except Exception:
                            continue
                    
                    results["specific_cell"] = {
                        "found": True,
                        "total_count": len(pmids),
                        "overexpression": {"articles": specific_oe[:5], "count": len(specific_oe)},
                        "knockdown": {"articles": specific_kd[:5], "count": len(specific_kd)},
                        "knockout": {"articles": specific_ko[:5], "count": len(specific_ko)},
                        "message": f"åœ¨ {cell_line} ä¸­æ‰¾åˆ° {len(pmids)} ç¯‡æ–‡çŒ®"
                    }
                else:
                    results["specific_cell"] = {"found": False, "message": f"æœªåœ¨ {cell_line} ä¸­æ‰¾åˆ°ç›¸å…³ç ”ç©¶"}
            except Exception as e:
                results["specific_cell"] = {"found": False, "message": f"æ£€ç´¢å¤±è´¥: {e}"}
        else:
            results["specific_cell"] = {"found": False, "message": "æœªè¾“å…¥ç»†èƒç³»åç§°"}
        
        # åˆ†æ”¯2ï¼šé€šç”¨æ–‡çŒ®
        for construct_type in ["overexpression", "knockdown", "knockout"]:
            type_map = {
                "overexpression": "overexpression OR over-expression OR ectopic",
                "knockdown": "knockdown OR siRNA OR shRNA",
                "knockout": "knockout OR CRISPR OR knock-out"
            }
            
            query = f'{gene_symbol}[Title/Abstract] AND ({type_map[construct_type]}) AND (cell line OR cell-line)'
            
            try:
                handle = Entrez.esearch(db="pubmed", term=query, retmax=100, sort="relevance")
                record = Entrez.read(handle)
                handle.close()
                
                pmids = record["IdList"]
                articles_list = []
                
                if pmids:
                    fetch_ids = pmids[:10]
                    handle = Entrez.efetch(db="pubmed", id=fetch_ids, rettype="abstract", retmode="xml")
                    articles = Entrez.read(handle)
                    handle.close()
                    
                    for article in articles.get("PubmedArticle", []):
                        try:
                            medline = article["MedlineCitation"]
                            article_data = medline["Article"]
                            title = article_data.get("ArticleTitle", "N/A")
                            pmid = str(medline.get("PMID", "N/A"))
                            abstract = str(article_data.get("Abstract", {}).get("AbstractText", ""))
                            
                            methods = [kw for kw in ["lentiviral", "transfection", "electroporation"] 
                                      if kw in (title + abstract).lower()]
                            
                            articles_list.append({
                                "pmid": pmid,
                                "title": title,
                                "methods": ", ".join(methods) if methods else "æœªæ˜ç¡®æåŠ",
                                "abstract_snippet": abstract[:300] + "..." if len(abstract) > 300 else abstract
                            })
                        except Exception:
                            continue
                
                results[construct_type] = {"count": len(pmids), "articles": articles_list}
                
            except Exception as e:
                results[construct_type] = {"count": 0, "articles": [], "error": str(e)}
        
        return results
    
    def _format_database_record(self, gene_symbol: str, species: str, cell_line: Optional[str],
                               ncbi_info: Dict, uniprot_info: Dict, lentiviral: Dict,
                               literature: Dict, plasmids: List, hpa_gene_data: Dict) -> Dict:
        """æ ¼å¼åŒ–æ•°æ®åº“è®°å½•"""
        return {
            "gene_symbol": gene_symbol,
            "species": species,
            "cell_line": cell_line,
            "gene_id": ncbi_info.get("gene_id"),
            "uniprot_id": uniprot_info.get("uniprot_id"),
            "ensembl_id": hpa_gene_data.get("ensembl_id"),
            "cds_length": uniprot_info.get("cds_length_bp"),
            "is_essential": ncbi_info.get("phenotype") == "å¿…éœ€ï¼ˆæ½œåœ¨è‡´æ­»é£é™©ï¼‰",
            "lentiviral_suitable": lentiviral.get("overall_suitable"),
            "lentiviral_risk": lentiviral.get("function_risk", {}).get("risk_level"),
            "literature_count": {
                "oe": literature.get("overexpression", {}).get("count", 0),
                "kd": literature.get("knockdown", {}).get("count", 0),
                "ko": literature.get("knockout", {}).get("count", 0)
            },
            "plasmid_count": len(plasmids),
            "hpa_data_available": bool(hpa_gene_data),
            "timestamp": datetime.now().isoformat()
        }


# ==================== Streamlit UI ====================

def main():
    st.title("ğŸ”¬ æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V1")
    st.markdown("**2000bpåŒ…è£…æé™ç‰ˆ + åŠŸèƒ½é£é™©è¯„ä¼° + åºåˆ—æå–**")
    
    with st.sidebar:
        st.header("âš™ï¸ åˆ†æå‚æ•°è®¾ç½®")
        
        gene_symbol = st.text_input("åŸºå› ç¬¦å· (Gene Symbol)", "TP53").strip().upper()
        
        species = st.selectbox(
            "åŸºå› æ¥æºç‰©ç§",
            ["Homo sapiens", "Mus musculus", "Rattus norvegicus"],
            index=0
        )
        
        cell_line = st.text_input("ç›®çš„ç»†èƒç³» (å¯é€‰)", "HEK293").strip()
        
        cell_species = st.selectbox(
            "ç»†èƒç³»ç‰©ç§",
            ["æœªæŒ‡å®š", "Human", "Mouse", "Rat", "Other"],
            index=0
        )
        
        analyze_btn = st.button("ğŸš€ å¼€å§‹æ·±åº¦åˆ†æ", type="primary", use_container_width=True)
        
        st.divider()
        
        hpa_checker = HPAGeneData()
        if hpa_checker.check_data_available():
            st.success("âœ… HPA äººç±»è›‹ç™½æ•°æ®å·²åŠ è½½")
        else:
            st.warning("âš ï¸ HPA æ•°æ®æœªé…ç½®")
            st.caption("é¦–æ¬¡ä½¿ç”¨å°†è‡ªåŠ¨ä¸‹è½½ï¼ˆçº¦30MBï¼‰")
        
        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        - âœ… NCBI/UniProt/HPA å¤šæºæ•°æ®
        - âœ… CDSé•¿åº¦ + åŠŸèƒ½é£é™©åŒè¯„ä¼°
        - âœ… æ–‡çŒ®åŒ…è£…è¯æ®æ£€ç´¢
        - âœ… shRNA/siRNA/sgRNAåºåˆ—æå–
        - âœ… Addgeneè´¨ç²’æŸ¥è¯¢
        - âœ… AIæ™ºèƒ½åˆ†æ
        """)
    
    if analyze_btn and gene_symbol:
        analyzer = ConstructAnalyzer()
        
        try:
            result = analyzer.analyze_gene(
                gene_symbol=gene_symbol,
                species=species,
                cell_line=cell_line if cell_line else None,
                cell_species=cell_species if cell_species != "æœªæŒ‡å®š" else None
            )
            
            st.session_state.analysis_results = result
            st.session_state.search_history.append({
                "gene": gene_symbol,
                "cell": cell_line,
                "time": datetime.now().strftime("%H:%M")
            })
            
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            st.exception(e)
    
    if st.session_state.analysis_results:
        display_results(st.session_state.analysis_results)

def display_results(result: Dict):
    """å±•ç¤ºåˆ†æç»“æœ"""
    info = result["input_info"]
    
    st.header(f"ğŸ“Š {info['gene_symbol']} åˆ†ææŠ¥å‘Š")
    cols = st.columns(4)
    cols[0].metric("åŸºå› ", info['gene_symbol'])
    cols[1].metric("ç‰©ç§", info['species'].split()[0])
    cols[2].metric("ç»†èƒç³»", info['cell_line'])
    cols[3].metric("AIåˆ†æ", "å·²å¯ç”¨" if info['ai_enabled'] else "æœªå¯ç”¨")
    
    tabs = st.tabs(["ğŸ§¬ åŸºå› åŠŸèƒ½", "ğŸ¦  æ…¢ç—…æ¯’é£é™©è¯„ä¼°", "ğŸ“š æ–‡çŒ®ä¸åºåˆ—", "ğŸ§ª å®éªŒèµ„æº"])
    
    # Tab 1: åŸºå› åŠŸèƒ½
    with tabs[0]:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.subheader("NCBI Gene")
            gene_data = result["gene_function"]
            if gene_data.get("status") == "success":
                st.write(f"**åŸºå› ID:** {gene_data.get('gene_id')}")
                st.write(f"**æŸ“è‰²ä½“:** {gene_data.get('chromosome')}")
                st.write(f"**å¿…éœ€æ€§:** {gene_data.get('phenotype')}")
                with st.expander("åŠŸèƒ½æè¿°"):
                    st.write(gene_data.get("description", "æ— "))
            else:
                st.error(gene_data.get("error", "æ— æ³•è·å–åŸºå› ä¿¡æ¯"))
        
        with col2:
            st.subheader("UniProt")
            prot_data = result["protein_data"]
            if prot_data.get("status") == "success":
                st.write(f"**UniProt ID:** {prot_data.get('uniprot_id')}")
                st.write(f"**è›‹ç™½é•¿åº¦:** {prot_data.get('protein_length')} aa")
                st.write(f"**CDSé•¿åº¦:** {prot_data.get('cds_length_bp')} bp")
                if prot_data.get("protein_name"):
                    st.write(f"**è›‹ç™½åç§°:** {prot_data.get('protein_name')[:50]}...")
                with st.expander("äºšç»†èƒå®šä½"):
                    st.write(prot_data.get("subcellular_location", "æœªæ ‡æ³¨"))
            else:
                st.error(prot_data.get("error", "æ— æ³•è·å–è›‹ç™½ä¿¡æ¯"))
        
        with col3:
            st.subheader("HPA è›‹ç™½è¡¨è¾¾")
            hpa_data = result.get("hpa_gene_data", {})
            if hpa_data:
                st.write(f"**Ensembl:** {hpa_data.get('ensembl_id', 'N/A')}")
                st.write(f"**å¯é æ€§:** {hpa_data.get('reliability', 'N/A')}")
                if hpa_data.get('rna_tissue_specificity'):
                    rna = str(hpa_data['rna_tissue_specificity'])
                    if len(rna) > 50:
                        rna = rna[:47] + "..."
                    st.write(f"**RNAè¡¨è¾¾:** {rna}")
                with st.expander("äºšç»†èƒå®šä½"):
                    st.write(hpa_data.get('subcellular_location', 'æœªæ ‡æ³¨'))
                st.caption(f"[HPAè¯¦æƒ…é¡µ]({hpa_data.get('hpa_link', '#')})")
            else:
                if info['species'] == "Homo sapiens":
                    st.info("æœªæ‰¾åˆ°HPAæ•°æ®")
                else:
                    st.info("HPAä»…æ”¯æŒäººç±»åŸºå› ")
    
    # Tab 2: æ…¢ç—…æ¯’é£é™©è¯„ä¼°
    with tabs[1]:
        lv = result["lentiviral_assessment"]
        
        st.subheader("ç»¼åˆé£é™©è¯„ä¼°")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("CDSé•¿åº¦é£é™©", lv['cds_assessment']['level'].upper(), 
                   help=lv['cds_assessment']['reason'])
        col2.metric("åŠŸèƒ½é£é™©ç­‰çº§", lv['function_risk']['risk_level'].upper(),
                   help="åŸºäºåŸºå› åŠŸèƒ½æè¿°çš„é£é™©è¯„ä¼°")
        col3.metric("æ€»ä½“è¯„çº§", lv['overall_rating'].split()[0])
        
        # è­¦å‘Šä¿¡æ¯
        if lv['warnings']:
            st.error("**âš ï¸ é£é™©æç¤º:**")
            for warning in lv['warnings']:
                st.write(f"- {warning}")
        
        # å»ºè®®
        if lv['recommendations']:
            st.success("**ğŸ’¡ ä¸“å®¶å»ºè®®:**")
            for rec in lv['recommendations']:
                st.write(f"- {rec}")
        
        # åŠŸèƒ½é£é™©è¯¦æƒ…
        with st.expander("æŸ¥çœ‹åŠŸèƒ½é£é™©è¯¦æƒ…"):
            if lv['function_risk']['risks']:
                for risk in lv['function_risk']['risks']:
                    st.write(f"- {risk}")
            else:
                st.write("æœªæ£€æµ‹åˆ°ç‰¹æ®ŠåŠŸèƒ½é£é™©")
            st.info(f"**ç­–ç•¥å»ºè®®:** {lv['function_risk']['recommendation']}")
        
        # æ–‡çŒ®è¯æ®
        with st.expander("æŸ¥çœ‹æ–‡çŒ®åŒ…è£…è¯æ®"):
            ev = lv['literature_evidence']['evidence']
            for construct, data in ev.items():
                status = "âœ…" if data['available'] else "âŒ"
                st.write(f"{status} **{construct}**: {data['count']}ç¯‡æ–‡çŒ® ({data['method']})")
    
    # Tab 3: æ–‡çŒ®ä¸åºåˆ—
    with tabs[2]:
        literature = result["cell_line_constructs"]
        lv = result["lentiviral_assessment"]
        
        # æ–‡çŒ®åˆ—è¡¨
        if literature.get("specific_cell", {}).get("found"):
            st.success(literature["specific_cell"]["message"])
            
            subtabs = st.tabs(["è¿‡è¡¨è¾¾", "æ•²ä½", "æ•²é™¤"])
            types_map = ["overexpression", "knockdown", "knockout"]
            
            for tab, ctype in zip(subtabs, types_map):
                with tab:
                    data = literature["specific_cell"][ctype]
                    st.write(f"æ‰¾åˆ° {data['count']} ç¯‡ç›¸å…³æ–‡çŒ®")
                    
                    if data["articles"]:
                        for article in data["articles"]:
                            with st.expander(f"{article['title'][:80]}..."):
                                st.write(f"**PMID:** {article['pmid']}")
                                st.write(f"**æ–¹æ³•:** {article['methods']}")
                    
                    if "ai_analysis" in data:
                        st.markdown("**ğŸ¤– AI æ–¹æ³•å­¦åˆ†æ**")
                        ai_data = data["ai_analysis"]
                        st.write(ai_data.get("summary", ""))
        
        # æå–çš„åºåˆ—
        if lv.get('sequences'):
            st.divider()
            st.subheader("ğŸ§¬ æ–‡çŒ®æŠ¥é“çš„é¶ç‚¹åºåˆ—")
            
            for construct_type, seqs in lv['sequences'].items():
                if any(seqs.values()):
                    with st.expander(f"{construct_type.upper()} åºåˆ—", expanded=True):
                        for seq_type, entries in seqs.items():
                            if entries:
                                st.text(f"{seq_type.upper()} åºåˆ— (æ¥è‡ªæ–‡çŒ®):")
                                for entry in entries:
                                    cols = st.columns([2, 1, 3])
                                    cols[0].code(entry['sequence'])
                                    cols[1].caption(f"PMID: {entry['pmid']}")
                                    cols[2].caption(entry['title'][:40])
        
        # é€šç”¨æ–‡çŒ®ç»Ÿè®¡
        st.divider()
        st.subheader("é€šç”¨åŸºå› è¡¨è¾¾è°ƒæ§æ–‡çŒ®")
        cols = st.columns(3)
        for idx, ctype in enumerate(["overexpression", "knockdown", "knockout"]):
            with cols[idx]:
                count = literature[ctype]["count"]
                label = ctype.replace("overexpression", "OE").replace("knockdown", "KD").replace("knockout", "KO")
                st.metric(label, f"{count} ç¯‡")
    
    # Tab 4: å®éªŒèµ„æº
    with tabs[3]:
        st.subheader("Addgene è´¨ç²’èµ„æº")
        plasmids = result["addgene_plasmids"]
        
        if plasmids:
            for p in plasmids:
                st.write(f"**[{p['plasmid_id']}]** {p['name']}")
                st.caption(f"[æŸ¥çœ‹è¯¦æƒ…]({p['url']})")
                st.divider()
        else:
            st.info("æœªæ‰¾åˆ°ç›¸å…³è´¨ç²’")

if __name__ == "__main__":
    main()
