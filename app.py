# -*- coding: utf-8 -*-
"""
æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V1
- 2000bpåŒ…è£…æé™ç‰ˆ + åŒåˆ†æ”¯æ–‡çŒ®æ£€ç´¢
- æ•´åˆHPAäººç±»è›‹ç™½è¡¨è¾¾æ•°æ®ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
"""

import streamlit as st
import pandas as pd
import requests
from Bio import Entrez
from datetime import datetime
import time
import re
from typing import Dict, List, Optional, Tuple
import json
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import urllib.parse
from collections import Counter
import hashlib
import openai
import os
import io
import zipfile

# ==================== é…ç½®ä¸åˆå§‹åŒ– ====================
st.set_page_config(
    page_title="æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V1",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å®‰å…¨å¯†é’¥é…ç½®
try:
    NCBI_EMAIL = st.secrets["NCBI_EMAIL"]
    NCBI_API_KEY = st.secrets.get("NCBI_API_KEY", "")
    APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
    BAILIAN_API_KEY = st.secrets.get("BAILIAN_API_KEY") or st.secrets.get("DASHSCOPE_API_KEY", "")
    AI_MODEL = st.secrets.get("AI_MODEL", "qwen-plus")
except Exception:
    st.error("âš ï¸ è¯·å…ˆé…ç½® Secretsï¼ˆNCBI_EMAIL ç­‰ï¼‰")
    st.stop()

# å¯†ç ä¿æŠ¤
if APP_PASSWORD:
    if 'authenticated' not in st.session_state:
        st.session_state.authenticated = False
    if not st.session_state.authenticated:
        pwd = st.text_input("ğŸ”’ è¯·è¾“å…¥è®¿é—®å¯†ç ", type="password")
        if pwd == APP_PASSWORD:
            st.session_state.authenticated = True
            st.rerun()
        elif pwd:
            st.error("å¯†ç é”™è¯¯")
        st.stop()

# åˆå§‹åŒ– session state
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'search_history' not in st.session_state:
    st.session_state.search_history = []

# ==================== Addgene çˆ¬å–æ¨¡å— ====================

class AddgeneScraper:
    """Addgene è´¨ç²’çˆ¬å–å™¨ - ç²¾ç®€ç‰ˆï¼ˆä»…åŒ¹é…åŸºå› åï¼‰"""
    
    def __init__(self):
        self.base_url = "https://www.addgene.org"
        self.ua = UserAgent()
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        })
    
    @st.cache_data(ttl=86400, show_spinner=False)
    def search_plasmids(_self, gene_symbol: str, max_results: int = 5) -> List[Dict]:
        """æœç´¢ Addgene è´¨ç²’ - ä»…è¿”å›åŸºå› ååŒ¹é…çš„ç»“æœ"""
        try:
            query = urllib.parse.quote(f"{gene_symbol}")
            search_url = f"{_self.base_url}/search/?q={query}&type=plasmid"
            headers = {'User-Agent': _self.ua.random}
            time.sleep(1)
            
            response = _self.session.get(search_url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            plasmids = []
            
            result_items = (soup.find_all('article', class_='addgene-search-result') or 
                           soup.find_all('div', class_='search-result-item') or
                           soup.select('.plasmid-item'))
            
            if not result_items:
                result_items = soup.select('[data-testid="plasmid-card"]')
            
            for item in result_items[:max_results]:
                try:
                    plasmid_data = _self._parse_plasmid_card(item, gene_symbol)
                    if plasmid_data:
                        plasmids.append(plasmid_data)
                except Exception:
                    continue
            
            if not plasmids:
                plasmids = _self._search_by_gene_page(gene_symbol)
            
            return plasmids
            
        except Exception as e:
            st.error(f"Addgene çˆ¬å–é”™è¯¯: {str(e)}")
            return []
    
    def _parse_plasmid_card(self, card, gene_symbol: str) -> Optional[Dict]:
        """è§£æå•ä¸ªè´¨ç²’å¡ç‰‡ - ä»…æå–åŸºæœ¬ä¿¡æ¯å’ŒåŸºå› ååŒ¹é…"""
        try:
            link_tag = card.find('a', href=re.compile(r'/\d{5,6}/')) or card.find('a', href=True)
            if not link_tag:
                return None
            
            href = link_tag.get('href', '')
            plasmid_id_match = re.search(r'/(\d{5,6})/', href)
            if not plasmid_id_match:
                return None
            
            plasmid_id = plasmid_id_match.group(1)
            full_url = f"{self.base_url}{href}" if href.startswith('/') else href
            
            name_tag = card.find('h3') or card.find('h2') or card.find('a', class_='title')
            name = name_tag.get_text(strip=True) if name_tag else "Unknown"
            
            # ä¸¥æ ¼åŒ¹é…ï¼šåŸºå› åå¿…é¡»å‡ºç°åœ¨è´¨ç²’åç§°æˆ–æè¿°ä¸­
            name_lower = name.lower()
            desc = card.get_text().lower()
            gene_lower = gene_symbol.lower()
            
            if gene_lower not in name_lower and gene_lower not in desc:
                return None
            
            return {
                'plasmid_id': plasmid_id,
                'name': name,
                'url': full_url,
                'insert_gene': gene_symbol,
            }
            
        except Exception:
            return None
    
    def _search_by_gene_page(self, gene_symbol: str) -> List[Dict]:
        """é€šè¿‡åŸºå› ä¸“å±é¡µé¢æœç´¢"""
        try:
            gene_url = f"{self.base_url}/browse/gene/{gene_symbol}/"
            headers = {'User-Agent': self.ua.random}
            response = self.session.get(gene_url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'lxml')
            plasmids = []
            seen_ids = set()
            
            links = soup.find_all('a', href=re.compile(r'/\d{5,6}/'))
            for link in links[:5]:
                href = link.get('href', '')
                match = re.search(r'/(\d{5,6})/', href)
                if match:
                    pid = match.group(1)
                    if pid not in seen_ids:
                        seen_ids.add(pid)
                        name = link.get_text(strip=True) or f"{gene_symbol} related"
                        
                        if gene_symbol.lower() not in name.lower():
                            continue
                            
                        plasmids.append({
                            'plasmid_id': pid,
                            'name': name,
                            'url': f"{self.base_url}/{pid}/",
                            'insert_gene': gene_symbol,
                            'source': 'Gene page'
                        })
            return plasmids
        except Exception:
            return []


# ==================== HPA åŸºå› è¡¨è¾¾æ•°æ®æ¨¡å—ï¼ˆè‡ªåŠ¨ä¸‹è½½ç‰ˆï¼‰ ====================

class HPAGeneData:
    """
    åŸºäºæœ¬åœ° proteinatlas.tsv çš„äººç±»è›‹ç™½è¡¨è¾¾æ•°æ®æŸ¥è¯¢
    é¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨ä» Human Protein Atlas å®˜ç½‘ä¸‹è½½
    """
    
    def __init__(self, tsv_path: str = "data/proteinatlas.tsv"):
        self.tsv_path = tsv_path
        self.df = None
        self.available_columns = []
        
        # æœŸæœ›çš„åˆ—ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        self.desired_columns = {
            'Gene': 'Gene',
            'Ensembl': 'Ensembl',
            'Uniprot': 'Uniprot',
            'Subcellular main location': 'Subcellular main location',
            'Reliability': 'Reliability',
            'RNA tissue specific nTPM': 'RNA tissue specific nTPM'
        }
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½
        if not os.path.exists(self.tsv_path):
            self._auto_download()
        
        self._load_data()
    
    def _auto_download(self):
        """è‡ªåŠ¨ä¸‹è½½ HPA æ•°æ®æ–‡ä»¶ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰"""
        try:
            st.warning("â¬‡ï¸ é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½ HPA æ•°æ®æ–‡ä»¶ï¼ˆçº¦ 30MBï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
            
            os.makedirs("data", exist_ok=True)
            url = "https://www.proteinatlas.org/download/proteinatlas.tsv.zip"
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            response = requests.get(url, stream=True, timeout=300)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            chunk_size = 1024 * 1024
            
            zip_buffer = io.BytesIO()
            
            for chunk in response.iter_content(chunk_size=chunk_size):
                if chunk:
                    zip_buffer.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = min(downloaded / total_size, 1.0)
                        progress_bar.progress(progress)
                        status_text.text(f"ä¸‹è½½è¿›åº¦: {downloaded/1024/1024:.1f} MB / {total_size/1024/1024:.1f} MB")
            
            status_text.text("æ­£åœ¨è§£å‹...")
            
            zip_buffer.seek(0)
            with zipfile.ZipFile(zip_buffer, 'r') as zip_ref:
                zip_ref.extractall("data")
            
            progress_bar.empty()
            status_text.empty()
            
            if os.path.exists(self.tsv_path):
                st.success("âœ… HPA æ•°æ®ä¸‹è½½å®Œæˆï¼")
                time.sleep(1)
            else:
                raise FileNotFoundError("è§£å‹åæœªæ‰¾åˆ° proteinatlas.tsv")
                
        except Exception as e:
            st.error(f"âŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
    
    def _load_data(self):
        """åŠ è½½ TSV æ–‡ä»¶ï¼ˆè‡ªåŠ¨æ£€æµ‹åˆ—åï¼‰"""
        try:
            if not os.path.exists(self.tsv_path):
                st.warning("âš ï¸ HPA æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°")
                self.df = pd.DataFrame()
                return
            
            # å…ˆè¯»å–ç¬¬ä¸€è¡Œçœ‹çœ‹æœ‰å“ªäº›åˆ—
            sample_df = pd.read_csv(self.tsv_path, sep='\t', nrows=2)
            actual_columns = sample_df.columns.tolist()
            
            # æ‰¾å‡ºå®é™…å­˜åœ¨çš„åˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œå®¹å¿ç©ºæ ¼å·®å¼‚ï¼‰
            column_mapping = {}
            for desired_col in self.desired_columns.keys():
                # ç²¾ç¡®åŒ¹é…
                if desired_col in actual_columns:
                    column_mapping[desired_col] = desired_col
                else:
                    # å°è¯•ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
                    for actual_col in actual_columns:
                        if actual_col.lower().strip() == desired_col.lower().strip():
                            column_mapping[desired_col] = actual_col
                            break
            
            if not column_mapping:
                st.error("HPA æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœŸæœ›çš„åˆ—")
                self.df = pd.DataFrame()
                return
            
            # åªè¯»å–å­˜åœ¨çš„åˆ—
            cols_to_read = list(column_mapping.values())
            self.df = pd.read_csv(
                self.tsv_path, 
                sep='\t', 
                low_memory=False,
                usecols=cols_to_read,
                dtype=str  # å…¨éƒ¨ä½œä¸ºå­—ç¬¦ä¸²è¯»å–ï¼Œé¿å…ç±»å‹é—®é¢˜
            )
            
            # é‡å‘½åä¸ºæ ‡å‡†åç§°
            reverse_mapping = {v: k for k, v in column_mapping.items()}
            self.df = self.df.rename(columns=reverse_mapping)
            
            self.available_columns = list(column_mapping.keys())
            
            st.success(f"âœ… HPA æ•°æ®å·²åŠ è½½: {len(self.df):,} æ¡åŸºå›  (å¯ç”¨å­—æ®µ: {', '.join(self.available_columns)})")
            
        except Exception as e:
            st.error(f"åŠ è½½ HPA æ•°æ®å¤±è´¥: {e}")
            self.df = pd.DataFrame()
    
    def get_gene_data(self, gene_symbol: str) -> Dict:
        """è·å–æŒ‡å®šåŸºå› çš„è¡¨è¾¾æ•°æ®"""
        if self.df is None or self.df.empty:
            return {}
        
        try:
            matches = self.df[self.df['Gene'].str.upper() == gene_symbol.upper()]
            
            if matches.empty:
                return {}
            
            row = matches.iloc[0]
            
            # åªè¿”å›å®é™…å­˜åœ¨çš„å­—æ®µ
            result = {}
            if 'Ensembl' in self.available_columns:
                result['ensembl_id'] = str(row.get('Ensembl', ''))
            if 'Uniprot' in self.available_columns:
                result['uniprot_id'] = str(row.get('Uniprot', ''))
            if 'Subcellular main location' in self.available_columns:
                result['subcellular_location'] = str(row.get('Subcellular main location', ''))
            if 'Reliability' in self.available_columns:
                result['reliability'] = str(row.get('Reliability', ''))
            if 'RNA tissue specific nTPM' in self.available_columns:
                result['rna_tissue_specificity'] = str(row.get('RNA tissue specific nTPM', ''))
            
            # æ„å»ºé“¾æ¥ï¼ˆå¦‚æœæœ‰ Ensemblï¼‰
            if 'ensembl_id' in result and result['ensembl_id']:
                result['hpa_link'] = f"https://www.proteinatlas.org/{result['ensembl_id']}"
            
            return result
            
        except Exception as e:
            st.error(f"æŸ¥è¯¢ HPA åŸºå› æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def check_data_available(self) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦å¯ç”¨"""
        return self.df is not None and not self.df.empty


# ==================== NCBI æ•°æ®è·å–æ¨¡å— ====================

class BioDataFetcher:
    def __init__(self, email: str, api_key: str = ""):
        Entrez.email = email
        if api_key:
            Entrez.api_key = api_key
        self.uniprot_base = "https://rest.uniprot.org/uniprotkb/search.json"
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        self.addgene_scraper = AddgeneScraper()
        # å…³é”®ï¼šä½¿ç”¨ HPAGeneDataï¼ˆä¸æ˜¯ HumanAtlasScraperï¼‰
        self.hpa_data = HPAGeneData()
    
    def get_ncbi_gene_info(self, gene_symbol: str, species: str) -> Dict:
        try:
            term = f"{gene_symbol}[Gene Name] AND {species}[Organism]"
            handle = Entrez.esearch(db="gene", term=term, retmax=1)
            record = Entrez.read(handle)
            handle.close()
            
            if not record["IdList"]:
                return {"status": "not_found", "error": f"æœªæ‰¾åˆ° {gene_symbol} ({species})"}
            
            gene_id = record["IdList"][0]
            handle = Entrez.efetch(db="gene", id=gene_id, rettype="xml")
            gene_data = Entrez.read(handle)
            handle.close()
            
            gene_entry = gene_data[0]
            summary = gene_entry.get("Entrezgene_summary", "")
            
            lethal_keywords = ["essential", "lethal", "required for cell viability", 
                             "knockout mice die", "embryonic lethal"]
            phenotype = "éå¿…éœ€"
            if any(kw in summary.lower() for kw in lethal_keywords):
                phenotype = "å¿…éœ€ï¼ˆæ½œåœ¨è‡´æ­»é£é™©ï¼‰"
            
            return {
                "gene_id": gene_id,
                "symbol": gene_symbol,
                "species": species,
                "description": summary[:500] if summary else "æ— æè¿°",
                "phenotype": phenotype,
                "chromosome": gene_entry.get("Entrezgene_location", [{}])[0].get("Gene-location", {}).get("Gene-location_chromosome", "N/A"),
                "status": "success"
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def get_uniprot_info(self, gene_symbol: str, species: str) -> Dict:
        try:
            species_map = {"Homo sapiens": "human", "Mus musculus": "mouse", "Rattus norvegicus": "rat"}
            org = species_map.get(species, species.lower())
            
            query = f"gene:{gene_symbol}+organism:{org}"
            params = {
                "query": query,
                "fields": "accession,gene_names,length,cc_subcellular_location,sequence",
                "format": "json",
                "size": 1
            }
            
            response = requests.get(self.uniprot_base, params=params, headers=self.headers, timeout=10)
            data = response.json()
            
            if not data.get("results"):
                return {"status": "not_found", "error": f"UniProt æœªæ‰¾åˆ° {gene_symbol}"}
            
            protein = data["results"][0]
            accession = protein.get("primaryAccession", "")
            seq_length = protein.get("sequence", {}).get("length", 0)
            
            loc_text = ""
            comments = protein.get("comments", [])
            for comment in comments:
                if comment.get("commentType") == "SUBCELLULAR LOCATION":
                    locations = comment.get("subcellularLocations", [])
                    locs = [loc.get("location", {}).get("value", "") for loc in locations]
                    loc_text = "; ".join([l for l in locs if l])
            
            cds_length = seq_length * 3 if seq_length else 0
            
            return {
                "uniprot_id": accession,
                "protein_length": seq_length,
                "cds_length_bp": cds_length,
                "subcellular_location": loc_text or "æœªæ ‡æ³¨",
                "status": "success"
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}


# ==================== é€šä¹‰åƒé—® AI åˆ†ææ¨¡å— ====================

class AIAnalyzer:
    def __init__(self):
        self.client = None
        self.model = None
        self.cache = {}
        
        try:
            api_key = BAILIAN_API_KEY
            
            if not api_key:
                raise ValueError("æœªé…ç½® BAILIAN_API_KEY æˆ– DASHSCOPE_API_KEY")
            
            self.model = AI_MODEL
            
            self.client = openai.OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            
            st.success(f"âœ… é€šä¹‰åƒé—®å·²è¿æ¥ï¼ˆæ¨¡å‹ï¼š{self.model}ï¼‰")
            
        except Exception as e:
            st.error(f"é€šä¹‰åƒé—®åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            raise e
    
    def _get_cache_key(self, text: str, task: str) -> str:
        return hashlib.md5(f"{task}:{text}".encode()).hexdigest()[:16]
    
    def _call_qwen(self, prompt: str, json_mode: bool = True, temperature: float = 0.3) -> Dict:
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯èµ„æ·±ç»†èƒç”Ÿç‰©å­¦å’Œåˆ†å­å…‹éš†ä¸“å®¶ï¼Œæ“…é•¿åˆ†æåŸºå› åŠŸèƒ½ã€è®¾è®¡ç»†èƒç³»æ„å»ºå®éªŒæ–¹æ¡ˆã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸“ä¸šä¸”å…·ä½“ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": 2000
            }
            
            if json_mode:
                kwargs["response_format"] = {"type": "json_object"}
            
            with st.spinner("ğŸ¤– é€šä¹‰åƒé—®åˆ†æä¸­..."):
                response = self.client.chat.completions.create(**kwargs)
                content = response.choices[0].message.content
                
                if json_mode:
                    return json.loads(content)
                return {"result": content}
                
        except Exception as e:
            st.error(f"é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}")
            return {"error": str(e)}
    
    def assess_gene_essentiality(self, gene_data: Dict, uniprot_data: Dict) -> Dict:
        gene_name = gene_data.get("symbol", "")
        description = gene_data.get("description", "")
        location = uniprot_data.get("subcellular_location", "")
        
        cache_key = self._get_cache_key(f"{gene_name}_{description[:100]}", "essentiality")
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹åŸºå› ä¿¡æ¯ï¼Œåˆ†æè¯¥åŸºå› æ˜¯å¦ä¸ºç»†èƒå¿…éœ€åŸºå› ï¼ˆessential geneï¼‰ï¼Œå¹¶è¯„ä¼°æ„å»ºKOç»†èƒç³»çš„å¯è¡Œæ€§ã€‚

åŸºå› åç§°ï¼š{gene_name}
åŸºå› åŠŸèƒ½æè¿°ï¼š{description}
äºšç»†èƒå®šä½ï¼š{location}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_essential": "æ˜¯/å¦/å¯èƒ½",
    "confidence": "é«˜/ä¸­/ä½",
    "rationale": "è¯¦ç»†è§£é‡Šåˆ¤æ–­ç†ç”±ï¼ŒåŸºäºè¯¥åŸºå› çš„ç”Ÿç‰©å­¦åŠŸèƒ½",
    "expected_phenotype": "å¦‚æœæ•²é™¤ï¼Œé¢„æœŸä¼šå‡ºç°ä»€ä¹ˆç»†èƒè¡¨å‹ï¼ˆå¦‚ç»†èƒæ­»äº¡ã€å¢æ®–å‡æ…¢ã€å‘¨æœŸé˜»æ»ç­‰ï¼‰",
    "construction_difficulty": "å®¹æ˜“/ä¸­ç­‰/å›°éš¾",
    "recommendation": "å…·ä½“çš„å®éªŒå»ºè®®ï¼ˆå¦‚æ˜¯å¦å»ºè®®ä½¿ç”¨è¯±å¯¼å‹ç³»ç»Ÿã€å…ˆå°è¯•KDè¿˜æ˜¯ç›´æ¥KOï¼‰",
    "key_considerations": "å®éªŒè®¾è®¡ä¸­çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼ˆ2-3ç‚¹ï¼‰",
    "alternative_approaches": "å¦‚æœKOå›°éš¾ï¼Œå»ºè®®çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚ä½¿ç”¨siRNAã€è¯±å¯¼å‹æ•²é™¤ç­‰ï¼‰"
}}

æ³¨æ„ï¼š
1. å¦‚æœåŸºå› æ˜¯ç®¡å®¶åŸºå› ï¼ˆå¦‚GAPDHã€ACTBï¼‰æˆ–å‚ä¸DNAå¤åˆ¶/ä¿®å¤æ ¸å¿ƒåŠŸèƒ½ï¼Œé€šå¸¸åˆ¤å®šä¸ºå¿…éœ€
2. å¦‚æœæ˜¯ç™ŒåŸºå› æˆ–ä¿¡å·é€šè·¯åˆ†å­ï¼Œé€šå¸¸ä¸ºéå¿…éœ€ä½†å¯èƒ½æœ‰è¡¨å‹
3. è¯·ç»™å‡ºå…·ä½“ç”Ÿç‰©å­¦æœºåˆ¶è§£é‡Šï¼Œä¸è¦æ³›æ³›è€Œè°ˆ"""
        
        result = self._call_qwen(prompt, json_mode=True)
        self.cache[cache_key] = result
        return result
    
    def analyze_literature_deep(self, articles: List[Dict], gene: str, construct_type: str) -> Dict:
        if not articles:
            return {"summary": "æ— ç›¸å…³æ–‡çŒ®", "protocols": []}
        
        articles_text = "\n\n".join([
            f"æ–‡çŒ®{i+1}:\næ ‡é¢˜ï¼š{a['title']}\næ–¹æ³•å…³é”®è¯ï¼š{a['methods']}\næ‘˜è¦ç‰‡æ®µï¼š{a.get('abstract_snippet', '')}"
            for i, a in enumerate(articles[:5])
        ])
        
        cache_key = self._get_cache_key(articles_text[:500], f"lit_{construct_type}")
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        prompt = f"""ä½œä¸ºæ–¹æ³•å­¦ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹å…³äº"{gene}"åŸºå› {construct_type}ï¼ˆè¿‡è¡¨è¾¾/æ•²ä½/æ•²é™¤ï¼‰ç»†èƒç³»æ„å»ºçš„æ–‡çŒ®ã€‚

{articles_text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "summary": "è¿™äº›ç ”ç©¶çš„æ€»ä½“æ–¹æ³•å­¦è¶‹åŠ¿ï¼ˆä¸­æ–‡ï¼Œ2-3å¥è¯æ¦‚æ‹¬ï¼‰",
    "protocols": [
        {{
            "cell_line": "ä½¿ç”¨çš„ç»†èƒç³»",
            "vector_system": "å…·ä½“è½½ä½“åç§°ï¼ˆå¦‚pLKO.1, lentiCRISPRv2, pLVXç­‰ï¼‰",
            "delivery_method": "é€’é€æ–¹æ³•ï¼ˆå¦‚æ…¢ç—…æ¯’è½¬å¯¼ã€è„‚è´¨ä½“è½¬æŸ“Lipofectamine 2000/3000ã€ç”µè½¬ç­‰ï¼‰",
            "selection_method": "ç­›é€‰æ–¹æ³•å’Œæµ“åº¦ï¼ˆå¦‚Puromycin 2Î¼g/mL, G418 500Î¼g/mLç­‰ï¼‰",
            "validation_method": "éªŒè¯æ–¹æ³•ï¼ˆå¦‚Western Blotã€qPCRã€å…ç–«è§å…‰ã€æµå¼ç­‰ï¼‰",
            "efficiency": "æ•ˆç‡ä¿¡æ¯ï¼ˆå¦‚æœ‰æåŠï¼‰",
            "duration": "å®éªŒå‘¨æœŸï¼ˆå¦‚æœ‰æåŠï¼‰"
        }}
    ],
    "common_methods": "æœ€å¸¸è§çš„æ–¹æ³•ç»„åˆï¼ˆä¸­æ–‡æ€»ç»“ï¼‰",
    "critical_factors": "å½±å“å®éªŒæˆåŠŸçš„å…³é”®å› ç´ ï¼ˆ2-3ç‚¹ï¼‰",
    "troubleshooting": "å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ±¡æŸ“ã€æ•ˆç‡ä½ã€ç»†èƒæ­»äº¡ç­‰ï¼‰",
    "recommendation": "é’ˆå¯¹è¯¥åŸºå› {construct_type}çš„å…·ä½“æ“ä½œå»ºè®®"
}}

æ³¨æ„ï¼š
1. å¦‚æœæŸå­—æ®µåœ¨æ–‡çŒ®ä¸­æœªæåŠï¼Œå¡«å†™"æœªæåŠ"
2. ä¼˜å…ˆæå–å…·ä½“çš„è¯•å‰‚åç§°å’Œæµ“åº¦ï¼ˆå¦‚Lipofectamine 2000, Polybrene 8Î¼g/mLç­‰ï¼‰
3. å¦‚æœæåˆ°å¤šç§ç»†èƒç³»ï¼Œåˆ—å‡ºæœ€å¸¸ç”¨çš„2-3ç§"""
        
        result = self._call_qwen(prompt, json_mode=True)
        self.cache[cache_key] = result
        return result


# ==================== åˆ†æä¸»ç±» ====================

class ConstructAnalyzer:
    def __init__(self):
        self.fetcher = BioDataFetcher(NCBI_EMAIL, NCBI_API_KEY)
    
    def analyze_gene(self, gene_symbol: str, species: str, 
                    cell_line: Optional[str] = None, 
                    cell_species: Optional[str] = None) -> Dict:
        
        with st.spinner(f"ğŸ” æ­£åœ¨æ·±åº¦åˆ†æ {gene_symbol}..."):
            
            st.text("æ£€ç´¢ NCBI Gene...")
            ncbi_info = self.fetcher.get_ncbi_gene_info(gene_symbol, species)
            time.sleep(0.5)
            
            st.text("æ£€ç´¢ UniProt...")
            uniprot_info = self.fetcher.get_uniprot_info(gene_symbol, species)
            time.sleep(0.5)
            
            st.text("æ£€ç´¢ Addgene...")
            addgene_plasmids = self.fetcher.addgene_scraper.search_plasmids(gene_symbol)
            time.sleep(0.5)
            
            # ä»…äººç±»åŸºå› æŸ¥è¯¢HPAæ•°æ®
            hpa_gene_data = {}
            if species == "Homo sapiens":
                st.text("æ£€ç´¢ HPA è›‹ç™½è¡¨è¾¾æ•°æ®...")
                hpa_gene_data = self.fetcher.hpa_data.get_gene_data(gene_symbol)
            time.sleep(0.5)
            
            lentiviral = self._assess_lentiviral(ncbi_info, uniprot_info, addgene_plasmids)
            literature = self._search_all_constructs(gene_symbol, cell_line)
            
            ai_analysis = {}
            try:
                ai_analyzer = AIAnalyzer()
                
                with st.spinner("ğŸ¤– é€šä¹‰åƒé—®æ­£åœ¨æ·±åº¦åˆ†æ..."):
                    ai_analysis["gene_assessment"] = ai_analyzer.assess_gene_essentiality(
                        ncbi_info, uniprot_info
                    )
                    
                    # AIåˆ†æç‰¹å®šç»†èƒçš„æ–‡çŒ®ï¼ˆå¦‚æœæœ‰ï¼‰
                    if cell_line and literature.get("specific_cell", {}).get("found"):
                        for ctype in ["overexpression", "knockdown", "knockout"]:
                            if literature["specific_cell"][ctype]["articles"]:
                                ai_result = ai_analyzer.analyze_literature_deep(
                                    literature["specific_cell"][ctype]["articles"], gene_symbol, ctype
                                )
                                literature["specific_cell"][ctype]["ai_analysis"] = ai_result
                    
                    # AIåˆ†æé€šç”¨æ–‡çŒ®
                    for ctype in ["overexpression", "knockdown", "knockout"]:
                        if literature[ctype]["articles"]:
                            ai_result = ai_analyzer.analyze_literature_deep(
                                literature[ctype]["articles"], gene_symbol, ctype
                            )
                            literature[ctype]["ai_analysis"] = ai_result
                    
                    ai_analysis["enabled"] = True
                    
            except Exception as e:
                st.warning(f"AIåˆ†ææœªå¯ç”¨ï¼š{e}")
                ai_analysis["enabled"] = False
            
            result = {
                "input_info": {
                    "gene_symbol": gene_symbol,
                    "species": species,
                    "cell_line": cell_line or "æœªæŒ‡å®š",
                    "cell_species": cell_species or "æœªæŒ‡å®š",
                    "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
                    "ai_enabled": ai_analysis.get("enabled", False)
                },
                "gene_function": ncbi_info,
                "protein_data": uniprot_info,
                "hpa_gene_data": hpa_gene_data,
                "addgene_plasmids": addgene_plasmids,
                "lentiviral_assessment": lentiviral,
                "cell_line_constructs": literature,
                "ai_analysis": ai_analysis,
                "database_record": self._format_database_record(
                    gene_symbol, species, cell_line, ncbi_info, uniprot_info, 
                    lentiviral, literature, addgene_plasmids, hpa_gene_data
                )
            }
            
            return result
    
    def _assess_lentiviral(self, ncbi_info: Dict, uniprot_info: Dict, plasmids: List) -> Dict:
        """è¯„ä¼°æ…¢ç—…æ¯’é€‚ç”¨æ€§ - 2000bpæé™ç®€åŒ–ç‰ˆ"""
        warnings = []
        recommendations = []
        cds_len = uniprot_info.get("cds_length_bp", 0)
        
        # è‡´æ­»æ€§åˆ¤æ–­
        if ncbi_info.get("phenotype") == "å¿…éœ€ï¼ˆæ½œåœ¨è‡´æ­»é£é™©ï¼‰":
            warnings.append("ğŸš¨ å¿…éœ€åŸºå› ï¼Œæ•²é™¤å¯èƒ½å¯¼è‡´ç»†èƒè‡´æ­»ï¼Œå»ºè®®ä½¿ç”¨è¯±å¯¼å‹ç³»ç»Ÿ")
        
        # é•¿åº¦åˆ¤æ–­
        if cds_len > 2000:
            warnings.append(f"âš ï¸ é•¿åº¦è¶…é™ï¼ˆ{cds_len}bp > 2000bpï¼‰")
            warnings.append("ğŸ’¡ å»ºè®®ï¼šå»é™¤è§å…‰æ ‡ç­¾æˆ–ä½¿ç”¨split-vectorç³»ç»Ÿ")
            suitable = False
            rating = "âŒ ä¸æ¨èï¼ˆé•¿åº¦è¶…é™ï¼‰"
        else:
            suitable = True
            rating = "âœ… å¯æ¥å—"
        
        # Addgeneèµ„æºæç¤º
        if plasmids:
            recommendations.append(f"Addgene æä¾› {len(plasmids)} ä¸ªè´¨ç²’")
        
        return {
            "suitable": suitable,
            "cds_length": cds_len,
            "packaging_limit": 2000,
            "warnings": warnings,
            "recommendations": recommendations,
            "overall_assessment": rating
        }
    
    def _search_all_constructs(self, gene_symbol: str, cell_line: Optional[str]) -> Dict:
        """åŒåˆ†æ”¯æ–‡çŒ®æ£€ç´¢"""
        results = {}
        
        # åˆ†æ”¯1ï¼šç‰¹å®šç›®çš„ç»†èƒç³»æ„å»º
        if cell_line:
            query_specific = f'{gene_symbol}[Title/Abstract] AND {cell_line}[Title/Abstract] AND (cell line OR cell-line)'
            
            try:
                handle = Entrez.esearch(db="pubmed", term=query_specific, retmax=100, sort="relevance")
                record = Entrez.read(handle)
                handle.close()
                
                pmids = record["IdList"]
                
                if pmids:
                    fetch_ids = pmids[:20]
                    handle = Entrez.efetch(db="pubmed", id=fetch_ids, rettype="abstract", retmode="xml")
                    articles = Entrez.read(handle)
                    handle.close()
                    
                    specific_oe, specific_kd, specific_ko = [], [], []
                    
                    for article in articles.get("PubmedArticle", []):
                        try:
                            medline = article["MedlineCitation"]
                            article_data = medline["Article"]
                            title = article_data.get("ArticleTitle", "").lower()
                            abstract = str(article_data.get("Abstract", {}).get("AbstractText", "")).lower()
                            full_text = title + " " + abstract
                            
                            pmid = str(medline.get("PMID", "N/A"))
                            title_display = article_data.get("ArticleTitle", "N/A")
                            
                            methods = [kw for kw in ["lentiviral", "transfection", "electroporation"] if kw in full_text]
                            
                            article_info = {
                                "pmid": pmid,
                                "title": title_display,
                                "methods": ", ".join(methods) if methods else "æœªæ˜ç¡®æåŠ",
                                "cell_line": cell_line
                            }
                            
                            if any(kw in full_text for kw in ["overexpression", "over-expression", "ectopic"]):
                                specific_oe.append(article_info)
                            elif any(kw in full_text for kw in ["knockdown", "sirna", "shrna"]):
                                specific_kd.append(article_info)
                            elif any(kw in full_text for kw in ["knockout", "crispr", "knock-out"]):
                                specific_ko.append(article_info)
                                    
                        except Exception:
                            continue
                    
                    results["specific_cell"] = {
                        "found": True,
                        "total_count": len(pmids),
                        "overexpression": {"articles": specific_oe[:5], "count": len(specific_oe)},
                        "knockdown": {"articles": specific_kd[:5], "count": len(specific_kd)},
                        "knockout": {"articles": specific_ko[:5], "count": len(specific_ko)},
                        "message": f"åœ¨ {cell_line} ä¸­æ‰¾åˆ° {len(pmids)} ç¯‡æ–‡çŒ®ï¼ˆOE:{len(specific_oe)} KD:{len(specific_kd)} KO:{len(specific_ko)}ï¼‰"
                    }
                else:
                    results["specific_cell"] = {"found": False, "message": f"æœªåœ¨ {cell_line} ä¸­æ‰¾åˆ°ç›¸å…³ç ”ç©¶"}
            except Exception as e:
                results["specific_cell"] = {"found": False, "message": f"æ£€ç´¢å¤±è´¥: {e}"}
        else:
            results["specific_cell"] = {"found": False, "message": "æœªè¾“å…¥ç»†èƒç³»åç§°"}
        
        # åˆ†æ”¯2ï¼šé€šç”¨è¡¨è¾¾è°ƒæ§æ–‡çŒ®
        for construct_type in ["overexpression", "knockdown", "knockout"]:
            type_map = {
                "overexpression": "overexpression OR over-expression OR ectopic",
                "knockdown": "knockdown OR siRNA OR shRNA",
                "knockout": "knockout OR CRISPR OR knock-out"
            }
            
            query = f'{gene_symbol}[Title/Abstract] AND ({type_map[construct_type]}) AND (cell line OR cell-line)'
            
            try:
                handle = Entrez.esearch(db="pubmed", term=query, retmax=100, sort="relevance")
                record = Entrez.read(handle)
                handle.close()
                
                pmids = record["IdList"]
                articles_list = []
                
                if pmids:
                    fetch_ids = pmids[:10]
                    handle = Entrez.efetch(db="pubmed", id=fetch_ids, rettype="abstract", retmode="xml")
                    articles = Entrez.read(handle)
                    handle.close()
                    
                    for article in articles.get("PubmedArticle", []):
                        try:
                            medline = article["MedlineCitation"]
                            article_data = medline["Article"]
                            title = article_data.get("ArticleTitle", "N/A")
                            pmid = str(medline.get("PMID", "N/A"))
                            abstract = str(article_data.get("Abstract", {}).get("AbstractText", ""))
                            
                            methods = [kw for kw in ["lentiviral", "transfection", "electroporation"] 
                                      if kw in (title + abstract).lower()]
                            
                            articles_list.append({
                                "pmid": pmid,
                                "title": title,
                                "methods": ", ".join(methods) if methods else "æœªæ˜ç¡®æåŠ",
                                "abstract_snippet": abstract[:300] + "..." if len(abstract) > 300 else abstract
                            })
                        except Exception:
                            continue
                
                results[construct_type] = {"count": len(pmids), "articles": articles_list}
                
            except Exception as e:
                results[construct_type] = {"count": 0, "articles": [], "error": str(e)}
        
        return results
    
    def _format_database_record(self, gene_symbol: str, species: str, cell_line: Optional[str],
                               ncbi_info: Dict, uniprot_info: Dict, lentiviral: Dict,
                               literature: Dict, plasmids: List, hpa_gene_data: Dict) -> Dict:
        """æ ¼å¼åŒ–æ•°æ®åº“è®°å½•"""
        return {
            "gene_symbol": gene_symbol,
            "species": species,
            "cell_line": cell_line,
            "gene_id": ncbi_info.get("gene_id"),
            "uniprot_id": uniprot_info.get("uniprot_id"),
            "ensembl_id": hpa_gene_data.get("ensembl_id"),
            "cds_length": uniprot_info.get("cds_length_bp"),
            "is_essential": ncbi_info.get("phenotype") == "å¿…éœ€ï¼ˆæ½œåœ¨è‡´æ­»é£é™©ï¼‰",
            "lentiviral_suitable": lentiviral.get("suitable"),
            "literature_count": {
                "oe": literature.get("overexpression", {}).get("count", 0),
                "kd": literature.get("knockdown", {}).get("count", 0),
                "ko": literature.get("knockout", {}).get("count", 0)
            },
            "plasmid_count": len(plasmids),
            "hpa_data_available": bool(hpa_gene_data),
            "timestamp": datetime.now().isoformat()
        }


# ==================== Streamlit UI ====================

def main():
    st.title("ğŸ”¬ æ…¢ç—…æ¯’ä¸ç»†èƒç³»æ„å»ºæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ V1")
    st.markdown("**2000bpåŒ…è£…æé™ç‰ˆ + åŒåˆ†æ”¯æ–‡çŒ®æ£€ç´¢ + AI æ™ºèƒ½åˆ†æ**")
    
    with st.sidebar:
        st.header("âš™ï¸ åˆ†æå‚æ•°è®¾ç½®")
        
        gene_symbol = st.text_input("åŸºå› ç¬¦å· (Gene Symbol)", "TP53").strip().upper()
        
        species = st.selectbox(
            "åŸºå› æ¥æºç‰©ç§",
            ["Homo sapiens", "Mus musculus", "Rattus norvegicus"],
            index=0
        )
        
        cell_line = st.text_input("ç›®çš„ç»†èƒç³» (å¯é€‰)", "HEK293").strip()
        
        cell_species = st.selectbox(
            "ç»†èƒç³»ç‰©ç§",
            ["æœªæŒ‡å®š", "Human", "Mouse", "Rat", "Other"],
            index=0
        )
        
        analyze_btn = st.button("ğŸš€ å¼€å§‹æ·±åº¦åˆ†æ", type="primary", use_container_width=True)
        
        st.divider()
        
        # HPA æ•°æ®çŠ¶æ€æç¤º
        hpa_checker = HPAGeneData()  # ä¸´æ—¶å®ä¾‹æ£€æŸ¥çŠ¶æ€
        if hpa_checker.check_data_available():
            st.success("âœ… HPA äººç±»è›‹ç™½æ•°æ®å·²åŠ è½½")
        else:
            st.warning("âš ï¸ HPA æ•°æ®æœªé…ç½®")
            st.caption("é¦–æ¬¡ä½¿ç”¨å°†è‡ªåŠ¨ä¸‹è½½ï¼ˆçº¦30MBï¼‰")
        
        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        - âœ… NCBI/UniProt åŸºå› ä¿¡æ¯æ£€ç´¢
        - âœ… Addgene è´¨ç²’èµ„æºæŸ¥è¯¢
        - âœ… HPA äººç±»è›‹ç™½è¡¨è¾¾æ•°æ®ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
        - âœ… 2000bp æ…¢ç—…æ¯’åŒ…è£…æé™è¯„ä¼°
        - âœ… PubMed åŒåˆ†æ”¯æ–‡çŒ®æ£€ç´¢
        - âœ… é€šä¹‰åƒé—® AI æ™ºèƒ½åˆ†æ
        """)
    
    if analyze_btn and gene_symbol:
        analyzer = ConstructAnalyzer()
        
        try:
            result = analyzer.analyze_gene(
                gene_symbol=gene_symbol,
                species=species,
                cell_line=cell_line if cell_line else None,
                cell_species=cell_species if cell_species != "æœªæŒ‡å®š" else None
            )
            
            st.session_state.analysis_results = result
            st.session_state.search_history.append({
                "gene": gene_symbol,
                "cell": cell_line,
                "time": datetime.now().strftime("%H:%M")
            })
            
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
    
    # æ˜¾ç¤ºç»“æœ
    if st.session_state.analysis_results:
        display_results(st.session_state.analysis_results)

def display_results(result: Dict):
    """å±•ç¤ºåˆ†æç»“æœ"""
    info = result["input_info"]
    
    st.header(f"ğŸ“Š {info['gene_symbol']} åˆ†ææŠ¥å‘Š")
    cols = st.columns(4)
    cols[0].metric("åŸºå› ", info['gene_symbol'])
    cols[1].metric("ç‰©ç§", info['species'].split()[0])
    cols[2].metric("ç»†èƒç³»", info['cell_line'])
    cols[3].metric("AIåˆ†æ", "å·²å¯ç”¨" if info['ai_enabled'] else "æœªå¯ç”¨")
    
    tabs = st.tabs(["ğŸ§¬ åŸºå› åŠŸèƒ½", "ğŸ¦  æ…¢ç—…æ¯’è¯„ä¼°", "ğŸ“š æ–‡çŒ®æ£€ç´¢", "ğŸ§ª å®éªŒèµ„æº"])
    
    # Tab 1: åŸºå› åŠŸèƒ½ï¼ˆåŒ…å«HPAæ•°æ®ï¼‰
    with tabs[0]:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.subheader("NCBI Gene")
            gene_data = result["gene_function"]
            if gene_data.get("status") == "success":
                st.write(f"**åŸºå› ID:** {gene_data.get('gene_id')}")
                st.write(f"**æŸ“è‰²ä½“:** {gene_data.get('chromosome')}")
                st.write(f"**å¿…éœ€æ€§:** {gene_data.get('phenotype')}")
                with st.expander("åŠŸèƒ½æè¿°"):
                    st.write(gene_data.get("description", "æ— "))
            else:
                st.error(gene_data.get("error", "æ— æ³•è·å–åŸºå› ä¿¡æ¯"))
        
        with col2:
            st.subheader("UniProt")
            prot_data = result["protein_data"]
            if prot_data.get("status") == "success":
                st.write(f"**UniProt ID:** {prot_data.get('uniprot_id')}")
                st.write(f"**è›‹ç™½é•¿åº¦:** {prot_data.get('protein_length')} aa")
                st.write(f"**CDSé•¿åº¦:** {prot_data.get('cds_length_bp')} bp")
                with st.expander("äºšç»†èƒå®šä½"):
                    st.write(prot_data.get("subcellular_location", "æœªæ ‡æ³¨"))
            else:
                st.error(prot_data.get("error", "æ— æ³•è·å–è›‹ç™½ä¿¡æ¯"))
        
        with col3:
            st.subheader("HPA è›‹ç™½è¡¨è¾¾")
            hpa_data = result.get("hpa_gene_data", {})
            if hpa_data:
                st.write(f"**Ensembl:** {hpa_data.get('ensembl_id', 'N/A')}")
                st.write(f"**å¯é æ€§:** {hpa_data.get('reliability', 'N/A')}")
                rna_expr = hpa_data.get('rna_tissue_specificity', 'N/A')
                if len(str(rna_expr)) > 50:
                    rna_expr = str(rna_expr)[:47] + "..."
                st.write(f"**RNAè¡¨è¾¾:** {rna_expr}")
                with st.expander("äºšç»†èƒå®šä½"):
                    st.write(hpa_data.get('subcellular_location', 'æœªæ ‡æ³¨'))
                st.caption(f"[HPAè¯¦æƒ…é¡µ]({hpa_data.get('hpa_link', '#')})")
            else:
                if info['species'] == "Homo sapiens":
                    st.info("æœªæ‰¾åˆ°HPAæ•°æ®")
                else:
                    st.info("HPAä»…æ”¯æŒäººç±»åŸºå› ")
    
    # Tab 2: æ…¢ç—…æ¯’è¯„ä¼°
    with tabs[1]:
        lentiviral = result["lentiviral_assessment"]
        
        col1, col2, col3 = st.columns(3)
        col1.metric("CDSé•¿åº¦", f"{lentiviral['cds_length']} bp")
        col2.metric("åŒ…è£…æé™", "2000 bp")
        col3.metric("è¯„ä¼°ç»“æœ", lentiviral['overall_assessment'])
        
        if lentiviral['warnings']:
            st.error("**è­¦å‘Š:**")
            for warning in lentiviral['warnings']:
                st.write(warning)
        
        if lentiviral['recommendations']:
            st.info("**å»ºè®®:**")
            for rec in lentiviral['recommendations']:
                st.write(f"- {rec}")
        
        st.divider()
        st.markdown("""
        **2000bp æé™è¯´æ˜:**
        - ç¬¬ä¸‰ä»£æ…¢ç—…æ¯’ç³»ç»ŸåŒ…è£…å®¹é‡çº¦ 8kbï¼ˆå«è½½ä½“éª¨æ¶ï¼‰
        - å®é™…æ’å…¥ç‰‡æ®µå»ºè®® â‰¤2000bp ä»¥ç¡®ä¿æ»´åº¦
        - è¶…è¿‡ 2000bp å»ºè®®ï¼šä½¿ç”¨ split-vector ç³»ç»Ÿæˆ–é€‰æ‹©å…¶ä»–é€’é€æ–¹å¼
        """)
    
    # Tab 3: æ–‡çŒ®æ£€ç´¢
    with tabs[2]:
        literature = result["cell_line_constructs"]
        
        if literature.get("specific_cell", {}).get("found"):
            st.success(literature["specific_cell"]["message"])
            
            subtabs = st.tabs(["è¿‡è¡¨è¾¾", "æ•²ä½", "æ•²é™¤"])
            types_map = ["overexpression", "knockdown", "knockout"]
            
            for tab, ctype in zip(subtabs, types_map):
                with tab:
                    data = literature["specific_cell"][ctype]
                    st.write(f"æ‰¾åˆ° {data['count']} ç¯‡ç›¸å…³æ–‡çŒ®")
                    
                    if data["articles"]:
                        for article in data["articles"]:
                            with st.expander(f"{article['title'][:80]}..."):
                                st.write(f"**PMID:** {article['pmid']}")
                                st.write(f"**æ–¹æ³•:** {article['methods']}")
                    
                    if "ai_analysis" in data:
                        st.markdown("**ğŸ¤– AI æ–¹æ³•å­¦åˆ†æ**")
                        ai_data = data["ai_analysis"]
                        st.write(ai_data.get("summary", ""))
                        if ai_data.get("common_methods"):
                            st.info(f"å¸¸ç”¨æ–¹æ³•: {ai_data['common_methods']}")
        else:
            st.warning(literature.get("specific_cell", {}).get("message", "æœªæ£€ç´¢ç‰¹å®šç»†èƒç³»æ–‡çŒ®"))
        
        st.divider()
        st.subheader("é€šç”¨åŸºå› è¡¨è¾¾è°ƒæ§æ–‡çŒ®")
        
        cols = st.columns(3)
        for idx, ctype in enumerate(["overexpression", "knockdown", "knockout"]):
            with cols[idx]:
                count = literature[ctype]["count"]
                label = ctype.replace("overexpression", "OE").replace("knockdown", "KD").replace("knockout", "KO")
                st.metric(label, f"{count} ç¯‡")
    
    # Tab 4: å®éªŒèµ„æº
    with tabs[3]:
        st.subheader("Addgene è´¨ç²’èµ„æº")
        plasmids = result["addgene_plasmids"]
        
        if plasmids:
            for p in plasmids:
                st.write(f"**[{p['plasmid_id']}]** {p['name']}")
                st.caption(f"[æŸ¥çœ‹è¯¦æƒ…]({p['url']})")
                st.divider()
        else:
            st.info("æœªæ‰¾åˆ°ç›¸å…³è´¨ç²’")

if __name__ == "__main__":
    main()

